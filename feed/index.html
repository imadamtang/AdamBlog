<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>一博客</title><link>https://blog.imadamtang.cn/</link><description>巧伪不如拙诚</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>$../Theme/assets/AdamBlog.png</url><title>一博客</title><link>https://blog.imadamtang.cn/</link></image><language>zh-CN</language><lastBuildDate>Wed, 18 Jan 2023 20:37:00 +0806</lastBuildDate><pubDate>Wed, 18 Jan 2023 20:37:00 +0806</pubDate><item><title>Ceph从入门到反复入门</title><link>https://blog.imadamtang.cn/archives/Ceph%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%8F%8D%E5%A4%8D%E5%85%A5%E9%97%A8/</link><description>&lt;h2&gt;1. 概述&lt;/h2&gt;
&lt;h3&gt;1.1 定义&lt;/h3&gt;
&lt;p&gt;Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、开源的&lt;strong&gt;分布式的存储系统&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;详细内容可以参考&lt;a href="http://docs.ceph.org.cn/"&gt;官方文档&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;1.2 特点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ceph提供给了&lt;strong&gt;对象、块、文件存储&lt;/strong&gt;功能，适合海量数据的管理。&lt;/li&gt;
&lt;li&gt;有强大的伸缩性：供用户访问PB乃至EB级的数据&lt;/li&gt;
&lt;li&gt;Ceph存储集群织起了大量节点，它们之间靠相互通讯来复制数据、并动态地重分布数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;1.3 核心组件&lt;/h3&gt;
&lt;p&gt;Ceph的核心组件包括&lt;strong&gt;Ceph OSD&lt;/strong&gt;、&lt;strong&gt;Ceph Monitor&lt;/strong&gt;、&lt;strong&gt;Ceph MDS&lt;/strong&gt;三大组件。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ceph OSD&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Object Storage Device，主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其他OSD进行心跳检查，上报变化情况给Ceph Monitor。一般一个OSD对应一个硬盘（或分区），并对其进行管理。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ceph Monitor&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;负责监视Ceph集群，维护Ceph集群的健康状态、Cluster Map。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;Cluster Map是RADOS的关键数据结构，管理集群种的所有成员、关系、属性等信息，以及数据分发。&lt;/p&gt;&lt;p&gt;当用户需要存储数据到Ceph集群时，OSD先通过Monitor获取最新Map图，再根据Map图和Object id等，计算出数据最终存储位置。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ceph MDS&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;元数据：&lt;/p&gt;&lt;p&gt;又称为中介数据、中继数据。主要描述数据属性的信息，用来支持如指示存储位置、历史数据、资源查找、文件记录等功能。简言之，就是&lt;strong&gt;用于描述一个文件特征的系统数据&lt;/strong&gt;，比如访问权限、文件拥有者以及文件数据库的分布信息（inode)等等。在集群文件系统中，分布信息包括文件在磁盘上的位置以 及磁盘在集群中的位置。用户需要操作一个文件就必须首先得到它的元数据，才能定位到文件的位置并且得到文件的内容或相关属性。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Managers&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ceph Manager守护进程（ceph-mgr）负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;Ceph Manager守护进程还基于python的插件来管理和公开Ceph集群信息，包括基于Web的Ceph Manager Dashboard和 REST API。高可用性通常至少需要两个管理器。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3&gt;1.4 架构&lt;/h3&gt;
&lt;figure style="flex: 69.54813359528487" &gt;&lt;img loading="lazy" width="708" height="509" src="https://s2.ax1x.com/2020/03/05/37683j.png" /&gt;&lt;figcaption&gt;架构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Ceph可分为四个层级&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;（1）基础存储系统RADOS&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RADOS (Reliable, Autonomic, Distributed Object Store) 意为可靠、自动、分布式的对象存储，这一层是Ceph的基础，即用来存储用户数据一层。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;（2）基础库librados&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对RADOS抽象、封装，向上层提供API，便于直接基于RADOS进行开发。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;RADOS采用C++开发，原生librados API包括C和C++两类。librados和基于其开发的应用在物理上位于同一台机器，也因此被称为本地API。应用通过本地调用librados API，再由librados API通过socket与RADOS集群中的节点通信完成操作。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;（3）高层应用接口&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RADOS GW (RADOS Gateway)&lt;/strong&gt; ：是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RADOS GW提供的API抽象层次更高，但功能则不如librados强大。因此，开发者应针对自己的需求选择使用。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RBD (Reliable Block Device)&lt;/strong&gt;：提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;**（4）Ceph FS **&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ceph FS(Ceph File System) ，即Ceph集群的文件管理系统。通过Linux内核客户端和FUSE来提供一个兼容POSIX的文件系统。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;RADOS的存储逻辑架构&lt;/strong&gt;&lt;/p&gt;&lt;figure style="flex: 73.32848837209302" &gt;&lt;img loading="lazy" width="1009" height="688" src="https://s2.ax1x.com/2020/03/05/372cKs.png" /&gt;&lt;figcaption&gt;RADOS系统结构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;RADOS集群主要有两种节点：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;OSD（负责数据存储和维护）&lt;/li&gt;
&lt;li&gt;Monitor（系统状态检测和维护）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OSD和Monitor之间相互传输节点状态信息，形成Cluster Map。Cluster Map这个数据结构和RADOS提供的特定算法相结合，实现了Ceph“无需查表，算算就好”的核心机制和若干优秀特性。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/63725901"&gt;如何“算”出存储位置：CRUSH&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;客户端程序通过Cluster Map获取节点状态信息，然后在本地计算出OSD位置，之后直接与OSD通信，完成操作。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;由此可见，只要保证Cluster Map不频繁更新，则客户端完全不需要查表，也不依赖于任何元数据服务器，就可以访问OSD。RADOS运行过程中，Cluster Map的更新取决于系统状态的变化，只有两种：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;OSD出现故障。&lt;/li&gt;
&lt;li&gt;RADOS规模扩大。正常情况下，这两种事件发生的频率显然远远低于客户端对数据进行访问的频率。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2&gt;2. Ceph工作原理及流程&lt;/h2&gt;
&lt;h3&gt;2.1 RADOS对象寻址&lt;/h3&gt;
&lt;p&gt;Ceph存储集群，从Ceph客户端接收数据并存储为对象。对象是文件系统中的一个文件，存储在OSD上，由Ceph OSD守护进程处理OSD上的读/写操作。通常来说，一块磁盘和该磁盘对应的守护进程称为一个OSD。&lt;/p&gt;&lt;p&gt;传统架构中，客户端与一个中心化组件通信（网关、API、中间件等），可能导致单点故障并限制了性能与伸缩性。&lt;/p&gt;&lt;p&gt;Ceph消除了集中网关，允许客户端直接和Ceph OSD守护进程通讯。同时Ceph会自动在其他Ceph节点创建对象副本来确保数据安全和高可用性。此外，为了保证高可用性，Ceph Monitor也实现了集群化。该功能基于CRUSH算法。&lt;/p&gt;&lt;figure style="flex: 71.64502164502164" &gt;&lt;img loading="lazy" width="993" height="693" src="https://s2.ax1x.com/2020/03/08/3vIpTK.png" /&gt;&lt;figcaption&gt;Ceph寻址流程图&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;File&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户存储/访问的文件。用户存储数据到Ceph集群时，存储数据会被分割成多个object。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;object&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ceph存储的最小存储单元。每个object都有一个object id，每个object的大小是可以设置的，默认是4MB。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;PG&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Placement Group，对object的存储进行组织和位置映射。PG是一个逻辑概念，类似于数据库中的索引表，它的数量是固定的，不会随着OSD增加或减少而改变。每个object最后都会通过CRUSH计算映射到某个PG中，一个PG可以包含多个object。而PG和OSD之间则是“多对多”的映射关系。进行数据迁移时，Ceph会以PG为单位进行迁移，不会直接操作对象。PG存在的意义是提高ceph存储系统的性能和扩展性。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;PG是一种间址，PG的数量有限，记录PG跟OSD间的映射关系可行，而记录object到OSD之间的映射因为数量巨大而实际不可行或效率太低。从用途来说，搞个映射本身不是目的，&lt;strong&gt;让故障或者负载均衡变得可操作是目的。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;OSD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Object Storage Device，PG通过CRUSH算法映射到OSD中存储。如果是二副本的，则每个PG都会映射到二个osd，比如[osd.1,osd.2]，那么osd.1是存放该PG的主副本，osd.2是存放该PG的从副本，保证了数据的冗余。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;流程&lt;/strong&gt;&lt;/p&gt;&lt;figure style="flex: 43.687943262411345" &gt;&lt;img loading="lazy" width="616" height="705" src="https://s2.ax1x.com/2020/03/10/8FDSJg.jpg" /&gt;&lt;figcaption&gt;结构&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;将file切片为多个object，每个object有一个oid。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;切割规则本质上就是按照object的大小来切file，优点：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;object大小一致便于RADOS高效管理&lt;/li&gt;
&lt;li&gt;file的串行处理变成object的并行处理&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;oid = ino (identity number) + ono (object number)，ino是file的id，ono是object的id，前者用于文件定位，后者用于文件切片位置的定位。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;一个或多个object映射到PG中，映射规则：&lt;code&gt;oid = hash(oid) % m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ceph利用静态hash函数将oid集合映射为近似均匀分布的伪随机值集合，并且集合中每个oid都与PG的数量去模得到PG序号 (PGid)。PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好的提升CEPH集群的性能并使数据均匀分布。&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;hash完成均匀分布&lt;/li&gt;
&lt;li&gt;取模随机选取&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因为只有大量object和PG，这种伪随机关系的近似均匀才能成立，故要注意两点：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;object的size要合理配置，使得file能被切分成足够多的object&lt;/li&gt;
&lt;li&gt;PG数量应该为OSD总数的数百倍，才能保证有足够数量的PG提供映射。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;一个PG映射到n个OSD中（称为n副本，常用3副本），PGid通过CRUSH计算，得到n个OSD，其上运行的OSD deamon负责执行映射到本地的object在本地文件系统中的存储、访问、元数据维护等操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.2 数据的操作流程&lt;/h3&gt;
&lt;p&gt;模拟file的写入流程，为便于理解先假设如下前提：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;file较小，无需切分，故被映射到一个object上&lt;/li&gt;
&lt;li&gt;三副本情况&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 56.31868131868132" &gt;&lt;img loading="lazy" width="410" height="364" src="https://s2.ax1x.com/2020/03/09/8pnLqS.png" /&gt;&lt;figcaption&gt;file写入流程&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;流程描述&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;本地RADOS对象寻址，得到OSDs的id&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;client向ceph集群写入file时，先在本地进行RADOS对象寻址，将file转为object，并得到一组的3个OSDid，按照序号大小，分别为Primary OSD, Secondary OSD, Tertiary OSD。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;发起写入操作&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;client直接与Primary OSD通信，发起写入操作，Primary OSD接收到之后，并不立即执行写入操作，而先向Secondary OSD, Tertiary OSD发起写入操作，等待确认信息。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;回复确认信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Secondary OSD和Tertiary OSD完成写入操作后，回复Primary OSD确认信息，Primary OSD才执行写入操作，并回复client确认信息，至此完成写入操作。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;等待确认信息的过程会有较大的延时，因此，ceph可以分两次向client回复确认信息。&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;第一次，数据写入OSD内存缓冲区时，回复client。此时client可继续执行操作。&lt;/li&gt;
&lt;li&gt;第二次，数据从缓存写入硬盘后，再回复client。此时client可根据需要删除本地数据，从而完成写入操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;从上述流程可以看到，client寻址直接在本地完成，无需中心化组件。故client与OSD之间可以多对多并行通信，满足了分布式的要求。并且，OSD在各个PG中可能担任的角色不同，从而尽最大可能均分了工作负担，避免单个OSD的性能问题造成系统的性能瓶颈。&lt;/p&gt;&lt;p&gt;读取数据同理，client直接和Primary OSD通信，最终由Primary OSD返回读数据。&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;Q&amp;amp;A：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;读数据是否可以从任意从OSD读取而非仅仅从主OSD读取？&lt;/li&gt;
&lt;li&gt;或者是否可以从n个OSD并行读取呢？从而提高带宽利用率。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3&gt;2.3 集群维护&lt;/h3&gt;
&lt;p&gt;若干Monitor共同负责了Ceph集群所有OSD状态的发现与记录，并形成Cluster Map，并分发到各个OSD和client中。OSD利用Cluster Map进行数据的维护，client利用Cluster Map进行寻址操作。&lt;/p&gt;&lt;p&gt;Monitor之间为主从备份关系，Monitor并不主动轮询OSD（这样做时间开销太大），而是OSD主动上报状态信息。Monitor收到信息后更新Cluster Map。触发条件有：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;新OSD加入集群&lt;/li&gt;
&lt;li&gt;某个OSD发现自身&lt;/li&gt;
&lt;li&gt;其他OSD出现异常&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Cluster Map的组成&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Monitor Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含集群的 &lt;code&gt;fsid&lt;/code&gt; 、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监视器图，用 &lt;code&gt;ceph mon dump&lt;/code&gt; 命令。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;OSD Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含集群 &lt;code&gt;fsid&lt;/code&gt; 、创建时间、最近修改时间、存储池列表、副本数量、归置组数量、 OSD 列表及其状态（如 &lt;code&gt;up&lt;/code&gt; 、 &lt;code&gt;in&lt;/code&gt; ）。要查看OSD运行图，用 &lt;code&gt;ceph osd dump&lt;/code&gt; 命令。&lt;/p&gt;&lt;p&gt;OSD状态的描述分为两个维度：up或者down（表明OSD是否正常工作），in或者out（表明OSD是否承载PG）。因此，对于任意一个OSD，共有四种可能的状态：&lt;/p&gt;&lt;p&gt;—— Up且in：说明该OSD正常运行，且已经承载至少一个PG的数据。这是一个OSD的标准工作状态；&lt;/p&gt;&lt;p&gt;—— Up且out：说明该OSD正常运行，但并未承载任何PG，其中也没有数据。一个新的OSD刚刚被加入Ceph集群后，便会处于这一状态。而一个出现故障的OSD被修复后，重新加入Ceph集群时，也是处于这一状态；&lt;/p&gt;&lt;p&gt;—— Down且in：说明该OSD发生异常，但仍然承载着至少一个PG，其中仍然存储着数据。这种状态下的OSD刚刚被发现存在异常，可能仍能恢复正常，也可能会彻底无法工作；&lt;/p&gt;&lt;p&gt;—— Down且out：说明该OSD已经彻底发生故障，且已经不再承载任何PG。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;PG Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含归置组版本、其时间戳、最新的 OSD 运行图版本、占满率、以及各归置组详情，像归置组 ID 、 up set 、 acting set 、 PG 状态（如 &lt;code&gt;active+clean&lt;/code&gt; ），和各存储池的数据使用情况统计。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;CRUSH Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含存储设备列表、故障域树状结构（如设备、主机、机架、行、房间、等等）、和存储数据时如何利用此树状结构的规则。要查看 CRUSH 规则，执行 &lt;code&gt;ceph osd getcrushmap -o {filename}&lt;/code&gt; 命令；然后用 &lt;code&gt;crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}&lt;/code&gt; 反编译；然后就可以用 &lt;code&gt;cat&lt;/code&gt; 或编辑器查看了。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;MDS Map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;包含当前 MDS 图的版本、创建时间、最近修改时间，还包含了存储元数据的存储池、元数据服务器列表、还有哪些元数据服务器是 &lt;code&gt;up&lt;/code&gt; 且 &lt;code&gt;in&lt;/code&gt; 的。要查看 MDS 图，执行 &lt;code&gt;ceph mds dump&lt;/code&gt; 。&lt;/p&gt;&lt;h3&gt;2.4 增加OSD流程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;新OSD上线后，如果PG处于正常状态，则新OSD首先根据配置信息与monitor通信。Monitor将其加入cluster map，并设置为up且out状态，再将最新版本的cluster map发给这个新OSD。收到monitor发来的cluster map之后，这个新OSD计算出自己所承载的PG（为简化讨论，此处我们假定这个新的OSD开始只承载一个PG），以及和自己承载同一个PG的其他OSD。然后，新OSD将与这些OSD取得联系。如图：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 74.86535008976661" &gt;&lt;img loading="lazy" width="834" height="557" src="https://s2.ax1x.com/2020/03/09/8pYyu9.png" /&gt;&lt;figcaption&gt;PG处于正常状态，OSD未启用&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;新OSD上线后，如果PG处于降级状态（即承载该PG的OSD个数少于正常值，如正常应该是3个，此时只有2个或1个。这种情况通常是OSD故障所致），则其他OSD将把这个PG内的所有对象和元数据复制给新OSD。数据复制完成后，新OSD被置为up且in状态。而cluster map内容也将据此更新。这事实上是一个自动化的failure recovery过程。当然，即便没有新的OSD加入，降级的PG也将计算出其他OSD实现failure recovery，如图：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 73.29093799682035" &gt;&lt;img loading="lazy" width="922" height="629" src="https://s2.ax1x.com/2020/03/09/8ptQV1.png" /&gt;&lt;figcaption&gt;PG处于降级状态时&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;如果该PG目前一切正常，则这个新OSD将替换掉现有OSD中的一个（PG内将重新选出Primary OSD），并承担其数据。在数据复制完成后，新OSD被置为up且in状态，而被替换的OSD将退出该PG（但状态通常仍然为up且in，因为还要承载其他PG）。而cluster map内容也将据此更新。这事实上是一个自动化的数据re-balancing过程，如图：&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 89.0909090909091" &gt;&lt;img loading="lazy" width="1078" height="605" src="https://s2.ax1x.com/2020/03/09/8ptaqA.png" /&gt;&lt;figcaption&gt;PG处于正常状态，新旧OSD更换&lt;/figcaption&gt;&lt;/figure&gt;&lt;hr /&gt;
&lt;h2&gt;3. Ceph (Luminous)集群创建&lt;/h2&gt;
&lt;p&gt;一个Ceph集群至少需要3个OSD才能实现冗余和高可用性，并且至少需要一个mon和一个mds。其中mon和mds可以部署在其中一个osd节点上，详细可参考&lt;a href="http://docs.ceph.org.cn/start/"&gt;官方安装文档&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;3.1 服务器初始化&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;不要用CentOS 8！！！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不要用CentOS 8！！！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不要用CentOS 8！！！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不要问我怎么知道的，全是泪……&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;官方文档给出了建议的&lt;a href="http://docs.ceph.org.cn/start/os-recommendations/#id2"&gt;操作系统&lt;/a&gt;，不看官方文档的后果就是…吐血&lt;/p&gt;&lt;p&gt;本来想用虚拟机的，后来想到我在Vultr上还有100$赠送券没用，索性直接用VPS来实现了。&lt;/p&gt;&lt;p&gt;结果Vultr给我埋了个坑！厂商初始化的系统，只有一个vda磁盘，且所有的容量都挂载根分区了。而Ceph需要一个格式为xfs的单独分区，搜了下linux缩小根分区要进rescue模式，VPS又不好弄…&lt;/p&gt;&lt;p&gt;解铃还须系铃人，搜了半天，最终找到解决办法了。就是使用VPS厂商提供的&lt;strong&gt;Block Storage&lt;/strong&gt;功能来扩展出一个分区。不过最蛋疼的是这个功能支持&lt;strong&gt;纽约&lt;/strong&gt;机房的VPS，之前搞了半天西雅图的机房，又得重来...裂开了&lt;/p&gt;&lt;p&gt;扩展好分区之后，硬盘情况如下：&lt;/p&gt;&lt;figure style="flex: 79.37158469945355" &gt;&lt;img loading="lazy" width="581" height="366" src="https://s2.ax1x.com/2020/03/10/8i3wwR.png" /&gt;&lt;figcaption&gt;硬盘分区情况&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;各个节点配置如下：&lt;/p&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
  &lt;th&gt;主机名&lt;/th&gt;
  &lt;th&gt;系统&lt;/th&gt;
  &lt;th&gt;IP&lt;/th&gt;
  &lt;th&gt;数据磁盘&lt;/th&gt;
  &lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
  &lt;td&gt;ceph-client&lt;/td&gt;
  &lt;td&gt;CentOS 7 X64&lt;/td&gt;
  &lt;td&gt;140.82.15.88&lt;/td&gt;
  &lt;td&gt;25G&lt;/td&gt;
  &lt;td&gt;client&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;ceph-node1&lt;/td&gt;
  &lt;td&gt;CentOS 7 X64&lt;/td&gt;
  &lt;td&gt;104.207.133.105&lt;/td&gt;
  &lt;td&gt;25G&lt;/td&gt;
  &lt;td&gt;mgr.mon.osd.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;ceph-node2&lt;/td&gt;
  &lt;td&gt;CentOS 7 X64&lt;/td&gt;
  &lt;td&gt;45.63.5.140&lt;/td&gt;
  &lt;td&gt;25G&lt;/td&gt;
  &lt;td&gt;osd.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;ceph-node3&lt;/td&gt;
  &lt;td&gt;CentOS 7 X64&lt;/td&gt;
  &lt;td&gt;149.28.40.16&lt;/td&gt;
  &lt;td&gt;25G&lt;/td&gt;
  &lt;td&gt;osd.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;mgr：管理节点&lt;/p&gt;&lt;p&gt;mon：监控节点&lt;/p&gt;&lt;p&gt;osd：存储节点&lt;/p&gt;&lt;p&gt;Ceph集群要求必须是奇数个mon监控节点，一般建议至少是3个&lt;/p&gt;&lt;p&gt;Ceph的文件系统作为一个目录挂载到客户端cephclient的&lt;code&gt;/cephfs&lt;/code&gt;目录下，可以像操作普通目录一样对此目录进行操作。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3&gt;3.2 环境准备&lt;/h3&gt;
&lt;p&gt;ok，话不多说，先把环境配好。&lt;/p&gt;&lt;h4&gt;(1) 修改hostname并绑定主机名映射&lt;/h4&gt;
&lt;p&gt;这一步是为了统一名称，方便后面命令操作使用的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在4个主机上修改hostname&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# hostnamectl set-hostname ceph-client&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph2 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# hostnamectl set-hostname ceph-node1&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph3 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# hostnamectl set-hostname ceph-node2&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph4 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# hostnamectl set-hostname ceph-node3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;如果使用ssh登陆的远程VPS，设置之后可能没有刷新hostname，exit退出重新登陆即可&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;在3个节点机上添加hosts&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# vi /etc/hosts&lt;/span&gt;

&lt;span class="m"&gt;104&lt;/span&gt;.207.133.105 ceph-node1
&lt;span class="m"&gt;45&lt;/span&gt;.63.5.140 ceph-node2
&lt;span class="m"&gt;149&lt;/span&gt;.28.40.16 ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;连通性测试&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-client ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# ping -c 2 ceph-node1&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-client ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# ping -c 2 ceph-node2&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-client ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# ping -c 2 ceph-node3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure style="flex: 150.87719298245614" &gt;&lt;img loading="lazy" width="688" height="228" src="https://s2.ax1x.com/2020/03/10/8CgorR.png" /&gt;&lt;figcaption&gt;连通性测试成功&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;另外2个节点机同理执行如上操作&lt;/p&gt;&lt;h4&gt;(2) 创建ceph用户&lt;/h4&gt;
&lt;p&gt;我们一般不会再root下工作，所以创建一个&lt;code&gt;ceph&lt;/code&gt;用户来使用。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在3个节点机上创建用户ceph，密码统一为ceph&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# adduser ceph &amp;amp;&amp;amp; echo &amp;quot;ceph:ceph&amp;quot;|chpasswd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为每个ceph节点用户增加root权限&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# echo &amp;quot;ceph ALL = (root) NOPASSWD:ALL&amp;quot; | sudo tee /etc/sudoers.d/ceph &amp;amp;&amp;amp; chmod 0440 /etc/sudoers.d/ceph&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试root权限，&lt;code&gt;exit&lt;/code&gt;退回切换前用户&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# su - ceph &lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# sudo su -&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;(3) 安装NTP&lt;/h4&gt;
&lt;p&gt;建议3个节点机安装NTP，以免因时钟漂移导致故障（mon节点必须）&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# yum install ntp ntpdate ntp-doc -y &lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# systemctl restart ntpd&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# systemctl status ntpd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;(4) 关闭防火墙和Selinux&lt;/h4&gt;
&lt;p&gt;保证各个节点之间的联通，在开发阶段完全可以关闭防火墙，之后再重新打开配置。&lt;/p&gt;&lt;p&gt;SELinux是一种基于 域-类型 模型（domain-type）的强制访问控制（MAC）安全系统，说人话就是Linux的一个安全子系统，它是默认开启的 ，会对我们Ceph安装过程造成一定的影响，所以先关掉它。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关闭防火墙&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# systemctl stop firewalld&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# systemctl disable firewalld&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;关闭selinux&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# sed -i &amp;#39;s/SELINUX=enforcing/SELINUX=disabled/g&amp;#39; /etc/selinux/config &amp;amp;&amp;amp; setenforce 0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;(5) epel源配置&lt;/h4&gt;
&lt;p&gt;yum默认的源是国外的，因为众所周知的原因，它的速度可能会比较慢。所以我们换成国内的源。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;把软件包源加入软件仓库&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;用文本编辑器创建一个 YUM 库文件：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# vi /etc/yum.repos.d/ceph.repo&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;ceph&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ceph packages &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;$basearch&lt;/span&gt;
&lt;span class="nv"&gt;baseurl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://mirrors.aliyun.com/ceph/rpm-luminous/el7/&lt;span class="nv"&gt;$basearch&lt;/span&gt;
&lt;span class="nv"&gt;enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;gpgcheck&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;priority&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rpm-md
&lt;span class="nv"&gt;gpgkey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://mirrors.aliyun.com/ceph/keys/release.asc

&lt;span class="o"&gt;[&lt;/span&gt;ceph-noarch&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ceph noarch packages
&lt;span class="nv"&gt;baseurl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch
&lt;span class="nv"&gt;enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;gpgcheck&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;priority&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rpm-md
&lt;span class="nv"&gt;gpgkey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://mirrors.aliyun.com/ceph/keys/release.asc

&lt;span class="o"&gt;[&lt;/span&gt;ceph-source&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ceph &lt;span class="nb"&gt;source&lt;/span&gt; packages
&lt;span class="nv"&gt;baseurl&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS
&lt;span class="nv"&gt;enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="nv"&gt;gpgcheck&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rpm-md
&lt;span class="nv"&gt;gpgkey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;https://mirrors.aliyun.com/ceph/keys/release.asc
&lt;span class="nv"&gt;priority&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;缓存服务器的包信息&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# yum makecache&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;更新yum&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# yum update -y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;(5) mgr节点安装ceph-deploy&lt;/h4&gt;
&lt;p&gt;之前的操作，都需要在所有非客户端节点进行，这个只需要在作为mgr节点的&lt;code&gt;ceph-node1&lt;/code&gt;上操作即可。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;安装ceph-deploy管理工具&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# yum install -y ceph-deploy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;figure style="flex: 107.83582089552239" &gt;&lt;img loading="lazy" width="578" height="268" src="https://s2.ax1x.com/2020/03/10/8PmlKx.png" /&gt;&lt;figcaption&gt;ceph-deploy部署工具安装成功&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;(6) 配置SSH服务器&lt;/h4&gt;
&lt;p&gt;正因为 &lt;code&gt;ceph-deploy&lt;/code&gt; 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点，完成节点间的免密码登陆。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;管理节点添加到各个节点机之间配置ssh服务器&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;根据官方文档建议，不要使用&lt;code&gt;root&lt;/code&gt;用户或&lt;code&gt;sudo&lt;/code&gt;，切换回&lt;code&gt;ceph&lt;/code&gt;用户。&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ su - ceph
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;新建密钥（一路回车即可）&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ssh-keygen
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;公钥分发，期间提示输入其他节点&lt;code&gt;ceph&lt;/code&gt;用户的密码，我们同样也是配置的&lt;code&gt;ceph&lt;/code&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ssh-copy-id  ceph@ceph-node2
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ssh-copy-id  ceph@ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;配置.ssh/config文件&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;修改 ceph-deploy 管理节点&lt;code&gt;ceph-node1&lt;/code&gt;上的 &lt;code&gt;~/.ssh/config&lt;/code&gt;文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 –username {username} 。这样做同时也简化了 ssh 的用法。&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ vi ~/.ssh/config

Host node1
   Hostname ceph-node1
   User ceph
Host node2
   Hostname ceph-node2
   User ceph
Host node3
   Hostname ceph-node3
   User ceph
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Host后为ssh连接的名称&lt;/p&gt;&lt;p&gt;Hostname接host文件中的名字，也是ceph-deploy所用的名字&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;重启ssh服务（需要输入root权限的密码）&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo chmod &lt;span class="m"&gt;600&lt;/span&gt; .ssh/config &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; systemctl restart sshd
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;测试ssh连接&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ssh node2
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node2 ~&lt;span class="o"&gt;]&lt;/span&gt;$ &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;3.3 创建集群&lt;/h3&gt;
&lt;h4&gt;(1) 创建ceph工作目录并配置ceph.conf&lt;/h4&gt;
&lt;p&gt;由于&lt;code&gt;ceph-deploy&lt;/code&gt;会在当前目录产生文件，故新建一个目录。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在mgr节点上创建ceph工作目录&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ mkdir my-cluster &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; my-cluster
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;创建ceph集群，&lt;code&gt;new&lt;/code&gt;后接mon节点的主机名。&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy new ceph-node1
&lt;/pre&gt;&lt;/div&gt;
&lt;figure style="flex: 201.11111111111111" &gt;&lt;img loading="lazy" width="724" height="180" src="https://s2.ax1x.com/2020/03/10/8iyORe.png" /&gt;&lt;figcaption&gt;安装成功&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;
&lt;p&gt;如果遇到python的import error：&lt;/p&gt;&lt;p&gt;这个问题通常是由于升级到python2.7后执行pip产生的，解决方案是重新在python2.7环境中安装pip，或者安装setuptools即可：&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo yum install -y python-setuptools
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;ceph-deploy&lt;/code&gt;的&lt;code&gt;new&lt;/code&gt;子命令能够部署一个默认名称为&lt;code&gt;ceph&lt;/code&gt;的新集群，并且它能生成集群配置文件和密钥文件。列出当前工作目录，你会看到&lt;code&gt;ceph.conf&lt;/code&gt;和&lt;code&gt;ceph.mon.keyring&lt;/code&gt;文件。&lt;/p&gt;&lt;figure style="flex: 288.8" &gt;&lt;img loading="lazy" width="722" height="125" src="https://s2.ax1x.com/2020/03/10/8icPmR.png" /&gt;&lt;figcaption&gt;产生的文件&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;
&lt;p&gt;如果需要(新安装的系统通常不需要)，部署之前确保ceph每个节点没有ceph数据包（先清空之前所有的ceph数据，如果是新装不用执行此步骤，如果是重新部署的话也执行下面的命令）&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy purge ceph-deploy ceph-u0-m0 ceph-u0-l0 ceph-u0-r0
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy purgedata ceph-deploy ceph-u0-m0 ceph-u0-l0 ceph-u0-r0
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy forgetkeys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;配置监控节点ceph.conf&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ vi ceph.conf
&lt;span class="c1"&gt;# 修改mon_host&lt;/span&gt;
&lt;span class="nv"&gt;mon_host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;104&lt;/span&gt;.207.133.105		//监控节点ip
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="http://docs.ceph.org.cn/rados/configuration/ceph-conf/"&gt;ceph.conf的配置文档&lt;/a&gt;&lt;/p&gt;&lt;h4&gt;(2) 节点安装ceph&lt;/h4&gt;
&lt;p&gt;在管理节点，利用ceph-deploy工具为各个节点安装ceph&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy install ceph-node1 ceph-node2 ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在各个节点检查安装情况&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph --version
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node2 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph --version
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node3 ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph --version
&lt;/pre&gt;&lt;/div&gt;
&lt;figure style="flex: 218.9655172413793" &gt;&lt;img loading="lazy" width="889" height="203" src="https://s2.ax1x.com/2020/03/10/8iWt5q.png" /&gt;&lt;figcaption&gt;安装情况检查&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;(3) mon节点初始化&lt;/h4&gt;
&lt;p&gt;初始化的时候，会生成mon节点监测集群所使用的秘钥&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy mon create-initial
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;本文在管理节点安装的mon，如果需要在其他节点安装mon，可使用如下命令：&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy mon node1 node2 node3 node4
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;将管理节点的配置分发到其它节点&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;收集所有密钥，OSD添加好后，用 &lt;code&gt;ceph-deploy&lt;/code&gt; 把配置文件和 管理节点密钥拷贝到管理节点和 Ceph 节点，这样每次执行 Ceph 命令行时就无需指定 mon节点 地址和 &lt;code&gt;ceph.client.admin.keyring&lt;/code&gt; 了&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将mon节点生成的密钥环分发到各个节点&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo scp *.keyring ceph-node1:/etc/ceph/
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo scp *.keyring ceph-node2:/etc/ceph/
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo scp *.keyring ceph-node3:/etc/ceph/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;允许&lt;code&gt;ceph&lt;/code&gt;用户访问&lt;code&gt;etc/ceph&lt;/code&gt;目录下的文件&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo chown -R ceph:ceph /etc/ceph
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;输入&lt;code&gt;ceph -s&lt;/code&gt;查看当前集群的状态信息&lt;/p&gt;&lt;figure style="flex: 64.59537572254335" &gt;&lt;img loading="lazy" width="447" height="346" src="https://s2.ax1x.com/2020/03/11/8AFMNQ.png" /&gt;&lt;figcaption&gt;ceph集群状态&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;(4) 配置mgr&lt;/h4&gt;
&lt;p&gt;mgr (manager)，用于管理集群&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy mgr create ceph-node1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;开启仪表盘&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph mgr module &lt;span class="nb"&gt;enable&lt;/span&gt; dashboard
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;通过&lt;code&gt;http://104.207.133.105:7000/&lt;/code&gt;查看集群信息&lt;/p&gt;&lt;figure style="flex: 146.7075038284839" &gt;&lt;img loading="lazy" width="1916" height="653" src="https://s2.ax1x.com/2020/03/11/8AVFVf.png" /&gt;&lt;figcaption&gt;管理界面&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;(5) 添加OSD到集群&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;检查OSD节点上所有可用的磁盘&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;使用zap选项删除所有osd节点上的分区&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy disk zap ceph-node1 /dev/vdb
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy disk zap ceph-node2 /dev/vdb
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy disk zap ceph-node3 /dev/vdb
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Ceph12版本以后prepare和activate命令已经没有了，用create替代&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;准备OSD（使用prepare命令）&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy osd prepare ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;激活OSD（使用activate命令）&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy osd activate ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;创建OSD&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;使用&lt;code&gt;ceph-deploy osd create --data {device} {ceph-node}&lt;/code&gt;创建OSD并添加到集群中&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy osd create --data /dev/vdb ceph-node1
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy osd create --data /dev/vdb ceph-node2
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-1 cluster&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy osd create --data /dev/vdb ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;figure style="flex: 68.31395348837209" &gt;&lt;img loading="lazy" width="470" height="344" src="https://s2.ax1x.com/2020/03/11/8An9fS.png" /&gt;&lt;figcaption&gt;添加OSD成功&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;ok，至此完工！可以去浏览器仪表盘直观地查看部署后的情况。&lt;/p&gt;&lt;h3&gt;3.4 集群管理&lt;/h3&gt;
&lt;p&gt;如果之前部署出了问题，可以用下面的命令予以修正。&lt;/p&gt;&lt;p&gt;检查集群状态&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ sudo ceph health
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;清除配置&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy purgedata ceph-node1 ceph-node2 ceph-node3
&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy forgetkeys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;清除ceph包&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;ceph@ceph-node1 cluster ~&lt;span class="o"&gt;]&lt;/span&gt;$ ceph-deploy purge ceph-node1 ceph-node2 ceph-node3
&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;引用&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.cnblogs.com/kevingrace/p/8387999.html"&gt;Ceph分布式存储工作原理 及 部署介绍-散尽浮华&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.cnblogs.com/kevingrace/p/9141432.html"&gt;Ceph-deploy快速部署Ceph分布式存储&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.cnblogs.com/linuxk/category/1267134.html"&gt;Ceph学习之路-烟雨浮华&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.cnblogs.com/passzhang/p/12151835.html"&gt;全网最详细的Ceph14.2.5集群部署及配置文件详解-PassZhang&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://blog.51cto.com/6854290/2361910"&gt;Centos7.6安装Ceph（luminous）-会飞的冬瓜&lt;/a&gt;&lt;/p&gt;</description><author>imadmtang@gmail.com (Adam)</author><guid isPermaLink="true">https://blog.imadamtang.cn/archives/Ceph%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%8F%8D%E5%A4%8D%E5%85%A5%E9%97%A8/</guid><pubDate>Sun, 03 May 2020 21:23:00 +0806</pubDate></item><item><title>吴恩达机器学习笔记</title><link>https://blog.imadamtang.cn/archives/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link><description>&lt;h2&gt;1 概述&lt;/h2&gt;
&lt;h3&gt;1.1 定义&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Arthur Samuel&lt;/em&gt;&lt;/p&gt;&lt;p&gt;“Field of study that gives computers the ability to learn without being explicitly programmed.”&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;机器学习：在没有进行特定编程的情况下给予计算机学习能力的领域。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Tom Mitchell&lt;/em&gt;&lt;/p&gt;&lt;p&gt;*“A computer program is said to &lt;em&gt;learn&lt;/em&gt; from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”*&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;机器学习：机器学习是为了执行&lt;strong&gt;任务T&lt;/strong&gt;，从&lt;strong&gt;经验E&lt;/strong&gt;中学习，用&lt;strong&gt;表现P&lt;/strong&gt;来评判它完成任务的性能，并从中提升。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;举例&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;过滤垃圾邮件&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用Tom的定义来说，T=正确分类垃圾邮件和普通邮件，E=查看你对邮件的人工标记（是否是垃圾邮件），P=正确分类的数量或概率。&lt;/p&gt;&lt;h3&gt;1.2 学习算法&lt;/h3&gt;
&lt;h4&gt;1.2.1 监督学习&lt;/h4&gt;
&lt;p&gt;监督学习：提供“正确答案”的数据集，来&lt;strong&gt;教&lt;/strong&gt;计算机如何学习&lt;/p&gt;&lt;h5&gt;1.2.1.1 回归问题&lt;/h5&gt;
&lt;p&gt;回归问题：预测出连续值属性的类型&lt;/p&gt;&lt;p&gt;&lt;strong&gt;举例&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;房价预测&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据已知的&lt;strong&gt;面积-房价&lt;/strong&gt;数据，得到一条较为拟合的函数（图中的紫色非线性函数，蓝色线性函数），通过回归预测出X面积房子的房价Y。&lt;/p&gt;&lt;figure style="flex: 88.65131578947368" &gt;&lt;img loading="lazy" width="1078" height="608" src="https://s2.ax1x.com/2020/01/02/lt0lw9.png" /&gt;&lt;figcaption&gt;房价预测&lt;/figcaption&gt;&lt;/figure&gt;&lt;h5&gt;1.2.1.2 分类问题&lt;/h5&gt;
&lt;p&gt;分类问题：预测出离散值属性的类型&lt;/p&gt;&lt;p&gt;&lt;strong&gt;举例&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;肿瘤预测&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据已知的&lt;strong&gt;年龄;肿瘤大小-良性/恶性&lt;/strong&gt;数据，得到一条拟合直线来区分良性和恶性，从而通过年龄和肿瘤大小来判断肿瘤是良性或恶性的（蓝圈良性，红叉恶性）。&lt;/p&gt;&lt;figure style="flex: 77.33812949640287" &gt;&lt;img loading="lazy" width="645" height="417" src="https://s2.ax1x.com/2020/01/02/lt08F1.png" /&gt;&lt;figcaption&gt;肿瘤预测&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过例子，我们了解到，我们需要通过事物的许多属性（房子面积、肿瘤大小和患者年龄等）来得到我们需要的结果（房价、良性或恶性）。属性是影响结果的一些关键性因素，而在实际中，我们影响事物的属性非常多，可计算机存储空间有限，不可能存放无限多的属性，所以我们要用到 &lt;strong&gt;支持向量机&lt;/strong&gt; 来解决这个问题。支持向量机通过一个简洁的数学技巧，允许计算基础护理无限多的属性。&lt;/p&gt;&lt;h4&gt;1.2.2 非监督学习&lt;/h4&gt;
&lt;p&gt;非监督学习：没有正确答案的数据集，让计算机从中自主找到某种蕴含的结构&lt;/p&gt;&lt;h5&gt;1.2.2.1 聚类算法&lt;/h5&gt;
&lt;p&gt;聚类算法：找出数据集中“蕴含”的分类&lt;/p&gt;&lt;p&gt;&lt;strong&gt;举例&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;肿瘤分类&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 57.35567970204842" &gt;&lt;img loading="lazy" width="616" height="537" src="https://s2.ax1x.com/2020/01/02/lt0GJx.png" /&gt;&lt;figcaption&gt;肿瘤分类&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;新闻分类&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 88.93728222996516" &gt;&lt;img loading="lazy" width="1021" height="574" src="https://s2.ax1x.com/2020/01/02/lt01oR.png" /&gt;&lt;figcaption&gt;新闻分类&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;鸡尾酒会算法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过Microphone #1和Microphone #2的声音，分离仅有出Speaker #1和Speaker #2的声音&lt;/p&gt;&lt;figure style="flex: 62.278978388998034" &gt;&lt;img loading="lazy" width="634" height="509" src="https://s2.ax1x.com/2020/01/02/ltscNj.png" /&gt;&lt;figcaption&gt;鸡尾酒会算法&lt;/figcaption&gt;&lt;/figure&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;]=&lt;/span&gt;&lt;span class="nb"&gt;svd&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;repmat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
#&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;svd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;奇异值分解&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;2 单特征线性回归&lt;/h2&gt;
&lt;h3&gt;2.1 定义&lt;/h3&gt;
&lt;p&gt;m=训练集样本数量&lt;/p&gt;&lt;p&gt;x=输入变量/特征&lt;/p&gt;&lt;p&gt;y=输出变量&lt;/p&gt;&lt;p&gt;h=映射函数(prediction)&lt;/p&gt;&lt;p&gt;轮廓图(contour plot)&lt;/p&gt;&lt;figure style="flex: 89.97975708502024" &gt;&lt;img loading="lazy" width="889" height="494" src="https://s2.ax1x.com/2020/01/07/l6dFgg.png" /&gt;&lt;figcaption&gt;轮廓图&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;
&lt;li&gt;相同颜色的每一圈表示$J(\theta_0,\theta_1)$值相等点集合&lt;/li&gt;
&lt;li&gt;$J(\theta_0,\theta_1)$最小值的点在圈最小的点集中&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;2.2 假设函数&lt;/h3&gt;
&lt;p&gt;$h_θ(x)=θ₀+θ₁x$&lt;/p&gt;&lt;figure style="flex: 92.63636363636364" &gt;&lt;img loading="lazy" width="1019" height="550" src="https://s2.ax1x.com/2020/01/07/l6lXo4.png" /&gt;&lt;figcaption&gt;线性回归&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;损失函数选择思路：选择一个函数$J(θ_0,θ_1)$，使得样本$y$与$h_θ(x)$之差最小。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;假设函数和损失函数的区别：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;假设函数是输入值和输出值的函数，是我们解决问题最终需要得到的函数，我们应该尽量使其与样本数据拟合。&lt;/p&gt;&lt;p&gt;损失函数是为了求解最优假设函数而设的函数，其目的是得到假设函数中的各个常量值。损失函数衡量了假设函数与实际问题之间的接近程度。&lt;/p&gt;&lt;h3&gt;2.3 损失函数&lt;/h3&gt;
&lt;h4&gt;2.3.1 平方差函数&lt;/h4&gt;
&lt;p&gt;平方差函数是基于：“离差平方和”，它反映了x与其数学期望Ex的偏离程度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Prediction:&lt;/strong&gt; $h_\theta(x)=\theta_0+\theta_1x$&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; $\theta_0,\theta_1$&lt;/p&gt;&lt;p&gt;**Cost Function: **$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; $J(\theta_0,\theta_1)min$&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;问：此处本来乘以1/m即可，为什么要乘以1/2m呢？&lt;/p&gt;&lt;p&gt;答：因为我们需要求$J_{min}$，利用梯度下降法对J求导时，如果是1/2m，求导结果为：，$\frac{∂J}{∂ {\theta}&lt;em&gt;i}=\frac{1}{m} \sum ^m _{i=1} (h&lt;/em&gt;\theta(x^{(i)})-y^{(i)}) \frac{∂h_{\theta}(x^{(i)})}{∂\theta}$正好求导时平方的2可以消去，简化了计算。&lt;/p&gt;&lt;p&gt;这里无论除以2m还是m，函数J最小值的θ都是相同的。&lt;/p&gt;&lt;/blockquote&gt;
&lt;figure style="flex: 88.98601398601399" &gt;&lt;img loading="lazy" width="1018" height="572" src="https://s2.ax1x.com/2020/01/07/l6YfoT.png" /&gt;&lt;figcaption&gt;平方差函数&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;2.4 损失函数最小化&lt;/h3&gt;
&lt;h4&gt;2.4.1 梯度下降法&lt;/h4&gt;
&lt;p&gt;梯度下降法的计算过程就是沿梯度下降/上升的方向求解极小值/极大值（局部/全局最优解）。&lt;/p&gt;&lt;p&gt;用于线性回归的损失函数都是“弓型”的，即凸函数，不存在局部最优解，一定存在并且仅有一个全局最优解。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;求解步骤：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;选择一个损失函数$J(\theta_0,\theta_1 )$&lt;/li&gt;
&lt;li&gt;初始化：$(\theta_0,\theta_1)=(0,0)$&lt;/li&gt;
&lt;li&gt;选择一个学习速率$α$ (即梯度下降的速度)&lt;/li&gt;
&lt;li&gt;开始迭代，令$\theta_j:=\theta_j-α \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$&lt;/li&gt;
&lt;li&gt;检查$\theta_j$是否趋于收敛（$\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1) \to 0$ 或 $\theta_j \to \theta_{j-1}$），若收敛则得到$(\theta_0,\theta_1)$，反之重复步骤3&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 88.96353166986565" &gt;&lt;img loading="lazy" width="927" height="521" src="https://s2.ax1x.com/2020/01/07/l6DnL4.png" /&gt;&lt;figcaption&gt;梯度下降求解&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;关于初始值：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;若损失函数存在多个局部最优解，则从不同的初始值进行梯度下降，可能得到不同的局部最优解。&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 98.59154929577464" &gt;&lt;img loading="lazy" width="560" height="284" src="https://s2.ax1x.com/2020/01/07/l6yc6K.png" /&gt;&lt;figcaption&gt;局部最优解&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;关于偏导：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;$\theta_0:=\theta_0-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)}) $&lt;/li&gt;
&lt;li&gt;$\theta_1:=\theta_1-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关于学习速率$α$ ：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;$α$如果过大，可能导致无法收敛；过小，可能导致求解速度太慢。&lt;/li&gt;
&lt;li&gt;$α$不需要在循环中改变大小的原因：因为越接近min，导数越接近0，所以导×$α$也会趋于0，故梯度下降算法会自动采取更小的下降步法。&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 92.41164241164242" &gt;&lt;img loading="lazy" width="889" height="481" src="https://s2.ax1x.com/2020/01/07/l6gIh9.png" /&gt;&lt;figcaption&gt;梯度下降步法自动减小&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;梯度下降的三种形式：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该三种形式的损失函数不同&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;批量梯度下降（Batch Gradient Descent，BGD）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$$&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repeat{
    θj:=θj−α1m∑mi=1(hθ(x(i))−y(i))x(i)j
    (for j =0,1)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;随机梯度下降（Stochastic Gradient Descent，SGD）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$J^{(i)}(\theta_0,\theta_1)=\frac{1}{2} (h_θ(x^{(i)})-y^{(i)})^2$$&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repeat{
    for i=1,...,m{
    	θj:=θj−α(hθ(x(i))−y(i))x(i)j
    	(for j =0,1)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;小批量梯度下降（Mini-Batch Gradient Descent, MBGD）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^k _{i=1} (h_θ(x^{(i)})-y^{(i)})^2 ,k\in(0,m)$$&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repeat{
    for i=1,11,21,31,...,991{
    	θj:=θj−α110∑(i+9)k=i(hθ(x(k))−y(k))x(k)j
    	(for j =0,1)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;参考：&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.cnblogs.com/lliuye/p/9451903.html"&gt;三种梯度下降形式的介绍&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.jianshu.com/p/c7e642877b0e"&gt;梯度下降理解&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;2.5 练习&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;根据分子中碳含量和燃烧释放能量的训练集，构建一个线性回归函数&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 66.39344262295081" &gt;&lt;img loading="lazy" width="1053" height="793" src="https://s2.ax1x.com/2020/01/07/lcyME4.png" /&gt;&lt;figcaption&gt;题目&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;问题解决思路&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;根据条件划出点集图，点集分布符合线性回归问题，故&lt;strong&gt;确认该问题类型&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;线性回归问题&lt;strong&gt;构建假设函数&lt;/strong&gt;：$h_\theta(x)=\theta_0+\theta_1*x$&lt;/li&gt;
&lt;li&gt;明确所需求参为 $\theta_0,\theta_1$ ，&lt;strong&gt;构建损失函数&lt;/strong&gt;，此处采用平方差函数：$J(\theta_0,\theta_1)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;明确最小化损失函数方法&lt;/strong&gt;，此处采用批量梯度下降法：&lt;ol&gt;
&lt;li&gt;初始化：$(\theta_0,\theta_1)=(0,0)$&lt;/li&gt;
&lt;li&gt;选择一个学习速率$α$ （常以0.001开始递增尝试）&lt;/li&gt;
&lt;li&gt;选择一个迭代次数iteraNum（常以1500开始）&lt;/li&gt;
&lt;li&gt;迭代过程中令$\theta_j:=\theta_j-α \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Octave实现代码如下：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;trainingset.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%定义梯度下降算法&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;theta&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;gradientDescent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X, y, alpha, iteraNum&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;iteraNum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="o"&gt;.*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%默认学习速率为0.01 迭代次数为5000次&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradientDescent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;xr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Training Data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Linear Regression&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Number of hydrocarbons in molecule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Heat release when burned(kJ/mol)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%theta = [-569,60; -530,91]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练后得到的回归函数为 $h_\theta(x)=-569.6 - 530.9*x$ :&lt;/p&gt;&lt;figure style="flex: 53.28185328185328" &gt;&lt;img loading="lazy" width="552" height="518" src="https://s2.ax1x.com/2020/01/07/lcgQUg.png" /&gt;&lt;figcaption&gt;效果图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;矩阵在该算法中的应用：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;对于训练集$(x_0,x_1,x_2,...,x_n,y)$，将$(x_0,x_1,x_2,...,x_n)$作为矩阵$X$，$(y)$作为矩阵(向量) $y$&lt;/li&gt;
&lt;li&gt;对损失函数的偏导$\frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)$ 包含 $\sum ^m &lt;em&gt;{i=1} (h_θ(x^{(i)})-y^{(i)})$ ，为保证&lt;strong&gt;同时更新&lt;/strong&gt;的要求，应先用矩阵乘法表示为：$dist=h&lt;/em&gt;\theta(x^{(i)})-y^{(i)}= \theta_0 + X\cdot\theta_1 - y$&lt;/li&gt;
&lt;li&gt;$\theta_1= \theta_1 - α&lt;em&gt;(1/m)&lt;/em&gt;sum(dist)$&lt;/li&gt;
&lt;li&gt;$\theta_2= \theta_2 - α&lt;em&gt;(1/m)&lt;/em&gt;sum(dist.&lt;em&gt;X)$ ，因为$J(\theta_1,\theta_2)$的两个偏导不同，对$\theta_2$的偏导需要再乘以$x$，故此处需要采用$dist.&lt;/em&gt;X$，其中的“ $.&lt;em&gt;$ ”是指同型矩阵$a_{ij}&lt;/em&gt;b_{ij}$&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2&gt;3 多特征线性回归&lt;/h2&gt;
&lt;h3&gt;3.1 定义&lt;/h3&gt;
&lt;p&gt;$m$=样本数量&lt;/p&gt;&lt;p&gt;$n$ = 特征数量&lt;/p&gt;&lt;p&gt;$x^{(i)}$=第i个样本&lt;/p&gt;&lt;p&gt;$x^{(i)}_j$=第i个样本的第j个特征&lt;/p&gt;&lt;h3&gt;3.2 假设函数&lt;/h3&gt;
&lt;p&gt;$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n $$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为了方便，令 $x_0=1$，即 $x^{(i)}_0=1$&lt;/li&gt;
&lt;li&gt;令 $x= \begin{bmatrix} x_0\x_1\x_2\\vdots\x_n\end{bmatrix} $ ， $\theta=\begin{bmatrix}\theta_0\\theta_1\\theta_2\\vdots\\theta_n\end{bmatrix}$ ，则 $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=\theta^Tx$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=\theta^Tx$&lt;/p&gt;&lt;figure style="flex: 187.1345029239766" &gt;&lt;img loading="lazy" width="640" height="171" src="https://s2.ax1x.com/2020/02/22/3QY2VO.png" /&gt;&lt;figcaption&gt;公式&lt;/figcaption&gt;&lt;/figure&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;prediciton&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;3.3 损失函数&lt;/h3&gt;
&lt;h4&gt;3.3.1 平方差函数&lt;/h4&gt;
&lt;p&gt;平方差函数是基于：“离差平方和”，它反映了x与其数学期望Ex的偏离程度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Prediction:&lt;/strong&gt; $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n =  X\theta$&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; $\theta_0,\theta_1,\cdots,\theta_n$ 也就是 $\theta^T=[\theta_0  \theta_1 \cdots \theta_n]$&lt;/p&gt;&lt;p&gt;**Cost Function: **$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; $J(\theta)min$&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sqrErrors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;		&lt;/span&gt;&lt;span class="c"&gt;%squared errors&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrErrors&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;3.4 损失函数最小化&lt;/h3&gt;
&lt;h4&gt;3.4.1 梯度下降法&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;求解步骤（完善）：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;选择一个损失函数$J(\theta)$&lt;/li&gt;
&lt;li&gt;初始化：$\theta=0$&lt;/li&gt;
&lt;li&gt;特征的缩放以及均值归一化&lt;/li&gt;
&lt;li&gt;选择一个学习速率$α$ ，画出 $J(\theta)$ — $iteraNum$ 的图像，找出最合适的学习速率&lt;/li&gt;
&lt;li&gt;开始迭代，令$\theta_j:=\theta_j-α\frac{\partial}{\partial\theta_j}J(\theta)$&lt;/li&gt;
&lt;li&gt;检查$\theta_j$是否趋于收敛（$\frac{\partial}{\partial \theta_j} J(\theta) \to 0$ 或 $\theta^j \to \theta^{j-1}$），若收敛则得到 $\theta$，反之重复步骤3&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;关于偏导：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;$\theta_0:=\theta_0-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)}) $&lt;/li&gt;
&lt;li&gt;$\theta_j:=\theta_j-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$\theta_j:=\theta_j-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;num_iters&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c"&gt;% X补充了一列1，所以可以直接乘X&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;3.4.2 正规方程法&lt;/h4&gt;
&lt;p&gt;采用数学中求解多项式函数的方法：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;求每个参数的偏导&lt;/li&gt;
&lt;li&gt;偏导置零得极值点&lt;/li&gt;
&lt;li&gt;极值点中选择出最小值点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基本思想如下：&lt;/p&gt;&lt;figure style="flex: 91.7536534446764" &gt;&lt;img loading="lazy" width="879" height="479" src="https://s2.ax1x.com/2020/01/19/1CdGM4.png" /&gt;&lt;figcaption&gt;θ的举证表示&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式： $\theta=(X^TX)^{-1}X^Ty$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;pinv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%pinv()返回矩阵伪逆阵&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对于极少数不可逆的矩阵$X^TX$，我们仍然用&lt;code&gt;pinv()&lt;/code&gt;函数。因为该函数是“伪逆”的，他在运算过程中与真正的求逆函数&lt;code&gt;inv()&lt;/code&gt;不同，会在一些步骤中有一点变动，但不影响结果。&lt;/p&gt;&lt;p&gt;矩阵不可逆的两个原因：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;当矩阵的两个特征选取不恰当时，可能会导致$X^TX$不可逆的情况出现。例如特征”英尺“和”米“作为两个特征，但实际上两个特征线性相关，则矩阵一定会不可逆。&lt;/li&gt;
&lt;li&gt;样本数&amp;lt;特征数时，可能会出现不可逆情况。此时就需要采用正则化等方法，使较少的样本仍然能得到有很多特征的参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;矩阵不可逆时，应先尝试排除冗余的特征。&lt;/p&gt;&lt;h4&gt;3.4.3 方法比较&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
  &lt;th&gt;梯度下降法&lt;/th&gt;
  &lt;th&gt;正规方程法&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
  &lt;td&gt;需要通过多次调试，找到合适的学习速率α&lt;/td&gt;
  &lt;td&gt;不需要学习速率α&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;需要多次迭代&lt;/td&gt;
  &lt;td&gt;不需要迭代&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;特征的数量对算法速度影响不大&lt;/td&gt;
  &lt;td&gt;θ包含$(X^TX)^{-1}$，时间复杂度$O(n^3)$，故特征不宜太多&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;经验：特征数量&amp;gt;1万时，速度将会有较大影响&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;3.5 特征选择&lt;/h3&gt;
&lt;p&gt;对于多个特征，例如房子的宽度和深度，可以设面积=宽度*深度，从而将特征减少。也即提醒，特征的选择应该遵循该特征是对结果有影响的。&lt;/p&gt;&lt;h4&gt;3.5.1 多项式回归&lt;/h4&gt;
&lt;p&gt;多项式回归使得我们可以采取线性回归的方法去拟合非常复杂甚至是非线性的函数。&lt;/p&gt;&lt;p&gt;如下图，将一元非线性多项式函数的n次方变量(n&amp;gt;1)，作为一个新特征值，从而变换成多元线性多项式函数。但需要注意的是，此时需要对n次方的变量进行特征缩放操作。&lt;/p&gt;&lt;figure style="flex: 92.02020202020202" &gt;&lt;img loading="lazy" width="911" height="495" src="https://s2.ax1x.com/2020/01/09/lWlTbt.png" /&gt;&lt;figcaption&gt;多项式替代非线性函数&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;3.6 特征处理&lt;/h3&gt;
&lt;h4&gt;3.5.1 归一化&lt;/h4&gt;
&lt;p&gt;若多特征值问题中，不同特征值在相近范围中时，采用特征缩放可使梯度下降更快地收敛。&lt;/p&gt;&lt;p&gt;$x_i=\frac{x_i}{s_i}$  其中$s_i$是$x_i$的取值范围大小$|max-min|$&lt;/p&gt;&lt;p&gt;下图中，针对“房屋面积”和“房间数量”两个特征，其相去甚远，轮廓图如左侧所示，采用梯度下降时，往往需要很长时间才能曲折找到收敛的值。当我们将特征值的取值范围缩放到[0,1]中时，轮廓图呈近似圆形，故此时进行梯度下降，将会更加快捷。&lt;/p&gt;&lt;figure style="flex: 88.9423076923077" &gt;&lt;img loading="lazy" width="925" height="520" src="https://s2.ax1x.com/2020/01/08/l2AuDJ.png" /&gt;&lt;figcaption&gt;特征缩放示意&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;实际中常缩放到[-1,1]中，但并非要求每个特征值都要缩放到该范围，只要在[-1,1] 的附近均可，例如[0,3] [-2,0.5]，本质只是将特征值范围的相差减小。同理，过小也需要放大。目前经验建议的范围是[-3,3] 至 [-1/3,1/3] 都无需缩放。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;均值归一化：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;将特征值的范围尽量关于0对称&lt;/p&gt;&lt;p&gt;$x_i=\frac{x_i-\mu_i}{s_i}$  其中$\mu_i$是$x_i$的平均值$\frac{\sum ^n _{i=1} {x_i}}{n}$&lt;/p&gt;&lt;h4&gt;3.4.2 标准化&lt;/h4&gt;
&lt;p&gt;将特征值转变为平均数为0，标准差为1的集合&lt;/p&gt;&lt;p&gt;$x_i=\frac{x_i-\mu_i}{\sigma}$ 其中$\sigma$是该特征集合的标准差&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;归一化和标准化的区别：&lt;/p&gt;&lt;p&gt;归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4&gt;3.4.3 调试&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;梯度下降正常工作的标准：&lt;/strong&gt; $J(\theta)$随着迭代次数的增加，单调递减，且趋于平缓。&lt;/p&gt;&lt;p&gt;画出 $J(\theta)$ — $iteraNum$ 的图像，下图为正常工作的图像。&lt;/p&gt;&lt;figure style="flex: 91.48514851485149" &gt;&lt;img loading="lazy" width="924" height="505" src="https://s2.ax1x.com/2020/01/09/lWuayq.png" /&gt;&lt;figcaption&gt;正常工作的Gradient-Descent&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下图为学习速率$α$不合适导致没有正常工作的图像。&lt;/p&gt;&lt;figure style="flex: 90.38076152304609" &gt;&lt;img loading="lazy" width="902" height="499" src="https://s2.ax1x.com/2020/01/09/lWuWOx.png" /&gt;&lt;figcaption&gt;非正常工作的Gradient-Descent&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;
&lt;li&gt;如果$α$太小，则收敛速度太慢。&lt;/li&gt;
&lt;li&gt;如果$α$太大，则可能无法迭代出最小值。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;故学习速率$α$可以通过上述 $J(\theta)$ — $iteraNum$ 图，来判断梯度下降算法是否能正常进行。&lt;/p&gt;&lt;h3&gt;3.7 线性回归总结及Code&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;根据连锁店在城市的欢迎程度，判断盈利&lt;/li&gt;
&lt;li&gt;根据数据集中，房子面积、房间数，判断房子出售价格&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;单特征线性回归&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;clear&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;all&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clc&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="s"&gt;inp_iterations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Please input iterations: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;	&lt;/span&gt;&lt;span class="c"&gt;%迭代次数&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;inp_alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Please input alpha: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;				&lt;/span&gt;&lt;span class="c"&gt;%学习速率&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 读取数据 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nLoading dataset...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ex1data1.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%特征+结果数（列数）&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%样本数（行数）&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%多特征的属性值矩阵X&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%结果向量&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 查看散点图（单特征） ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nPlotting Data...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MarkerSize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%大小为10的红色十字散点&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;description of x-axis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;description of y-axis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%kbhit();							%暂停&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 初始化 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nInitialing Variables...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%补充一列常数1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%初始化theta为n维向量&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;inp_iterations&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%迭代次数&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;inp_alpha&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;							&lt;/span&gt;&lt;span class="c"&gt;%学习速率&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 梯度下降法 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradientDescent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nTheta computed from gradient descent: \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; %f \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%kbhit();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 画出线性回归得出的线性函数（单特征） ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;hold&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;								&lt;/span&gt;&lt;span class="c"&gt;%在离散图中画线&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%X的2~n列用-线画出&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Training data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Linear regression&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;hold&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;off&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 学习速率-迭代次数关系图 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;numel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;LineWidth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Number of iterations&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cost J&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 梯度下降线性回归预测 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%梯度下降法，需要对参数进行相应的特征处理&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1650&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;		&lt;/span&gt;&lt;span class="c"&gt;%补充1列&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nPredicted profit ... \n (using gradient descent):\n $%f\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 损失函数调试（单特征） ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;checkJtheta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 正规方程法 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;normalEqn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nTheta computed from gradient descent: \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; %f \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%kbhit();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 正规方程线性回归预测 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%正规方程法直接代入参数即可&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1650&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nPredicted profit ... \n (using normal equations):\n $%f\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;多特征线性回归&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;clear&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;all&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;clc&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="s"&gt;inp_iterations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Please input iterations: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;	&lt;/span&gt;&lt;span class="c"&gt;%迭代次数&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;inp_alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Please input alpha: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;				&lt;/span&gt;&lt;span class="c"&gt;%学习速率&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 读取数据 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nLoading dataset...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ex1data2.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%特征+结果数（列数）&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%样本数（行数）&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%多特征的属性值矩阵X&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%结果向量&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 特征标准化（多特征） ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;featureNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%得到标准化的X以及均值和标准差&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 初始化 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nInitialing Variables...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%补充一列常数1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%初始化theta为n维向量&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;inp_iterations&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;						&lt;/span&gt;&lt;span class="c"&gt;%迭代次数&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;inp_alpha&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;							&lt;/span&gt;&lt;span class="c"&gt;%学习速率&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 梯度下降法 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gradientDescent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iterations&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nTheta computed from gradient descent: \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; %f \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%kbhit();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 梯度下降线性回归预测 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%梯度下降法，预测时需要对参数进行相应的特征处理&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1650&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre_norm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;							&lt;/span&gt;&lt;span class="c"&gt;%特征标准化&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre_norm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_pre_norm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;x_pre_norm&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;		&lt;/span&gt;&lt;span class="c"&gt;%补充1列&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x_pre_norm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nPredicted price ... \n (using gradient descent):\n $%f\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 学习速率-迭代次数关系图 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;numel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;-b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;LineWidth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Number of iterations&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cost J&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 正规方程法 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%消除特征标准化影响&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;					&lt;/span&gt;&lt;span class="c"&gt;%补充一列常数1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;normalEqn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nTheta computed from gradient descent: \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; %f \n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%kbhit();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;%% ========== 正规方程线性回归预测 ==========&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;%正规方程法直接代入参数即可&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1650&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x_pre&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nPredicted price ... \n (using normal equations):\n $%f\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;损失函数&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$J(\theta)=\frac{1}{2m}  \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})^2$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%该损失函数为平方差函数&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;J&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;computeCost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X, y, theta&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;				&lt;/span&gt;&lt;span class="c"&gt;% number of training examples&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;sqrErrors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;		&lt;/span&gt;&lt;span class="c"&gt;%squared errors&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqrErrors&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;特征标准化函数&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$x_i=\frac{x_i-\mu_i}{\sigma}$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;[X_norm, mu, sigma] = featureNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;				&lt;/span&gt;&lt;span class="c"&gt;%均值向量初始化&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%标准差向量初始化&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;    						&lt;/span&gt;&lt;span class="c"&gt;%计算每个特征的均值&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;    						&lt;/span&gt;&lt;span class="c"&gt;%计算每个特征的标准差&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;X_norm&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;    			&lt;/span&gt;&lt;span class="c"&gt;%计算标准化后的特征矩阵&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;梯度下降函数&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式：$\theta_j:=\theta_j-α \frac{1}{m} \sum ^m _{i=1} (h_θ(x^{(i)})-y^{(i)})\cdot x^{(i)}_j$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;[theta, J_history] = gradientDescent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X, y, theta, alpha, num_iters&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;% Initialize some useful values&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c"&gt;% number of training examples&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;num_iters&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;J_history&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;computeCost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;正规方程函数&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;依据公式： $\theta=(X^TX)^{-1}X^Ty$&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;[theta]&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;normalEqn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X, y&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;pinv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%pinv()返回矩阵伪逆阵&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;3D函数&amp;amp;轮廓图判断梯度下降性能函数（单特征）：&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%判断损失函数是否正常工作&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;checkJtheta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;X, y, theta&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nb"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\nVisualizing J(theta_0, theta_1) ...\n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;			&lt;/span&gt;&lt;span class="c"&gt;%theta0范围&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;				&lt;/span&gt;&lt;span class="c"&gt;%theta1范围&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;	&lt;/span&gt;&lt;span class="c"&gt;%初始化J为0阵&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;% 计算不同θ范围内，J的值&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;j&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)];&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;computeCost&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;% 曲面图&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;surf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\theta_0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\theta_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c"&gt;% 轮廓图&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;contour&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta0_vals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta1_vals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J_vals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\theta_0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;\theta_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;hold&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;on&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nb"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MarkerSize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;LineWidth&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;4 逻辑回归&lt;/h2&gt;
&lt;h3&gt;4.1 逻辑回归与线性回归关系&lt;/h3&gt;
&lt;p&gt;逻辑回归是分类问题，并非回归问题，但它是基于线性函数的。用来预测它属于某个分类的概率。&lt;/p&gt;&lt;p&gt;思考：我们之前学过线性回归，我们是否能通过线性回归解决问题呢？&lt;/p&gt;&lt;p&gt;如果按照线性回归的思路，我们可以通过人体内肿瘤的大小线性回归得到一个值，然后我们设置一个平均值或阈值，设为y。当线性回归出的值大于y则为恶性肿瘤，小于y则为良性肿瘤。于是，我们通过设置 &lt;strong&gt;平均值/阈值（决策边界）&lt;/strong&gt; ，将线性回归问题转化为了分类问题。如下图：&lt;/p&gt;&lt;figure style="flex: 111.19791666666667" &gt;&lt;img loading="lazy" width="427" height="192" src="https://s2.ax1x.com/2020/02/18/3FAsgJ.png" /&gt;&lt;figcaption&gt;用线性回归做分类问题&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图中，0.5为阈值，斜线为线性回归的假设函数，当值小于阈值时，为良性，反之为恶性。&lt;/p&gt;&lt;p&gt;但这个做法存在一个问题，当出现离群值（异常值）时，线性回归会受到很大的影响，如下图：&lt;/p&gt;&lt;figure style="flex: 145.43010752688173" &gt;&lt;img loading="lazy" width="541" height="186" src="https://s2.ax1x.com/2020/02/18/3FEWss.png" /&gt;&lt;figcaption&gt;线性回归出现离群值的情况&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从上图我们可以了解到，通过线性回归拟合出函数再取阈值分类的方法不一定有效，因为线性函数是“直”的，若存在离群值则对函数影响非常大。所以我们按照以下步骤做些处理：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;假设函数改用非线性函数&lt;/li&gt;
&lt;li&gt;去掉严重影响函数的离群值&lt;/li&gt;
&lt;li&gt;选定一个合适的阈值&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;4.2 假设函数的选择&lt;/h3&gt;
&lt;p&gt;首先，我们需要了解一个函数：&lt;strong&gt;Sigmoid Function (S形函数)&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;$S(x)=\frac{1}{1+e^{-x}}$  ，它是一个非线性函数&lt;/li&gt;
&lt;li&gt;$S^{'}(x)=\frac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x))$ ，它易于求导，单调递增，且反函数同样单增&lt;/li&gt;
&lt;li&gt;$S(0)=0.5$&lt;/li&gt;
&lt;li&gt;$S(x)\in(0,1)$ 在x趋于无穷时，函数接近于平滑，故对超过一定范围的输入值不敏感&lt;/li&gt;
&lt;li&gt;$S(x)$能将$(-∞,+∞)$的输入映射到$(0,1)$，从而可以作为概率的形式。&lt;/li&gt;
&lt;li&gt;可做逻辑回归中的二分类、神经网络的激活函数&lt;/li&gt;
&lt;/ol&gt;
&lt;figure style="flex: 288.9655172413793" &gt;&lt;img loading="lazy" width="838" height="145" src="https://s2.ax1x.com/2020/02/16/3pCpiq.png" /&gt;&lt;figcaption&gt;sigmoid函数图像&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;基于以上特点，我们在逻辑回归中便选择使用它作为假设函数：$h_\theta(x)=g(\theta^Tx)$&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;$z=\theta^Tx$&lt;/li&gt;
&lt;li&gt;$g(z)=\frac{1}{1+e^{−z}}$&lt;/li&gt;
&lt;li&gt;$h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta)$&lt;/li&gt;
&lt;li&gt;$P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1$&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;关于为什么选择的是Sigmoid函数，请参考&lt;a href="https://blog.csdn.net/qq_19645269/article/details/79551576"&gt;朱先生1994&lt;/a&gt;的博客&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3&gt;4.3 阈值的选择&lt;/h3&gt;
</description><author>imadmtang@gmail.com (Adam)</author><guid isPermaLink="true">https://blog.imadamtang.cn/archives/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid><pubDate>Mon, 12 Jul 2021 22:32:00 +0806</pubDate></item></channel></rss>