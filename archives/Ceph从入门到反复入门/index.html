<!DOCTYPE HTML>
<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Maverick,blog" />
    <meta name="generator" content="Maverick 1.2.1" />
    <meta name="template" content="Galileo" />
    <link rel="alternate" type="application/rss+xml" title="一博客 &raquo; RSS 2.0" href="https://blog.imadamtang.cn/feed/index.xml" />
    <link rel="alternate" type="application/atom+xml" title="一博客 &raquo; ATOM 1.0" href="https://blog.imadamtang.cn/feed/atom/index.xml" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/galileo-8d8763e752.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/ExSearch/ExSearch-182e5a8868.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/katex.min.css">
    <link href="https://fonts.googleapis.com/css?family=Fira+Code&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700&display=swap">
    <script>
        var ExSearchConfig = {
            root: "",
            api: "https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/dace093243ea820edb04a72acbaab809.json"
        }
    </script>
    
<title>Ceph从入门到反复入门 - 一博客</title>
<meta name="author" content="Adam" />
<meta name="description" content="1. 概述" />
<meta property="og:title" content="Ceph从入门到反复入门 - 一博客" />
<meta property="og:description" content="1. 概述" />
<meta property="og:site_name" content="一博客" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.imadamtang.cn/archives/Ceph%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%8F%8D%E5%A4%8D%E5%85%A5%E9%97%A8/" />
<meta property="og:image" content="" />
<meta property="article:published_time" content="2020-05-03T21:23:00+08.00" />
<meta name="twitter:title" content="Ceph从入门到反复入门 - 一博客" />
<meta name="twitter:description" content="1. 概述" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="" />


    
    </head>
    
    <body>
        
        <div class="container">
            <header id="ga-header">
                <div first>
                    <aside id="ga-brand">
                        <h1 class="brand"><a class="no-style" href="https://blog.imadamtang.cn/">一博客</a></h1>
                        <p>巧伪不如拙诚</p>
                    </aside>
                </div>
                <div second id="ga-nav">
                    <nav class="navs">
                        <ul><li><a class="ga-highlight" href="https://blog.imadamtang.cn/" target="_self">首页</a></li><span class="separator">·</span><li><a class="ga-highlight" href="https://blog.imadamtang.cn/archives/" target="_self">归档</a></li><span class="separator">·</span><li><a class="ga-highlight" href="https://blog.imadamtang.cn/about/" target="_self">关于</a></li><span class="separator">·</span><li><a href="#" target="_self" class="search-form-input ga-highlight">搜索</a></li></ul>
                    </nav>
                </div>
            </header>
            <div class="wrapper">
                
<main>    
    <section class="ga-section ga-content">
        <article class="yue">
            <h1 class="ga-post_title">Ceph从入门到反复入门</h1>
            <span class="ga-post_meta ga-mono">
                <span>Adam</span>
                <time>
                    2020-05-03
                </time>
                
                in <a no-style class="category" href="https://blog.imadamtang.cn/category/%E5%AD%A6%E4%B9%A0/">
                    学习
                </a>
                
                
                <span class="leancloud_visitors" 
                    id="/archives/Ceph%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%8F%8D%E5%A4%8D%E5%85%A5%E9%97%A8/" 
                    data-flag-title="Ceph从入门到反复入门"> · <i class="leancloud-visitors-count"></i> Views</span>
                
            </span>
            <div class="ga-content_body">
                <h2>1. 概述</h2>
<h3>1.1 定义</h3>
<p>Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、开源的<strong>分布式的存储系统</strong>。</p><p>详细内容可以参考<a href="http://docs.ceph.org.cn/">官方文档</a></p><h3>1.2 特点</h3>
<ul>
<li>Ceph提供给了<strong>对象、块、文件存储</strong>功能，适合海量数据的管理。</li>
<li>有强大的伸缩性：供用户访问PB乃至EB级的数据</li>
<li>Ceph存储集群织起了大量节点，它们之间靠相互通讯来复制数据、并动态地重分布数据。</li>
</ul>
<h3>1.3 核心组件</h3>
<p>Ceph的核心组件包括<strong>Ceph OSD</strong>、<strong>Ceph Monitor</strong>、<strong>Ceph MDS</strong>三大组件。</p><ul>
<li><strong>Ceph OSD</strong></li>
</ul>
<p>Object Storage Device，主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其他OSD进行心跳检查，上报变化情况给Ceph Monitor。一般一个OSD对应一个硬盘（或分区），并对其进行管理。</p><ul>
<li><strong>Ceph Monitor</strong></li>
</ul>
<p>负责监视Ceph集群，维护Ceph集群的健康状态、Cluster Map。</p><blockquote>
<p>Cluster Map是RADOS的关键数据结构，管理集群种的所有成员、关系、属性等信息，以及数据分发。</p><p>当用户需要存储数据到Ceph集群时，OSD先通过Monitor获取最新Map图，再根据Map图和Object id等，计算出数据最终存储位置。</p></blockquote>
<ul>
<li><strong>Ceph MDS</strong></li>
</ul>
<p>Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。</p><blockquote>
<p>元数据：</p><p>又称为中介数据、中继数据。主要描述数据属性的信息，用来支持如指示存储位置、历史数据、资源查找、文件记录等功能。简言之，就是<strong>用于描述一个文件特征的系统数据</strong>，比如访问权限、文件拥有者以及文件数据库的分布信息（inode)等等。在集群文件系统中，分布信息包括文件在磁盘上的位置以 及磁盘在集群中的位置。用户需要操作一个文件就必须首先得到它的元数据，才能定位到文件的位置并且得到文件的内容或相关属性。</p></blockquote>
<ul>
<li><strong>Managers</strong></li>
</ul>
<p>Ceph Manager守护进程（ceph-mgr）负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。</p><blockquote>
<p>Ceph Manager守护进程还基于python的插件来管理和公开Ceph集群信息，包括基于Web的Ceph Manager Dashboard和 REST API。高可用性通常至少需要两个管理器。</p></blockquote>
<h3>1.4 架构</h3>
<figure style="flex: 69.54813359528487" ><img loading="lazy" width="708" height="509" src="https://s2.ax1x.com/2020/03/05/37683j.png" /><figcaption>架构图</figcaption></figure><p>Ceph可分为四个层级</p><ul>
<li><strong>（1）基础存储系统RADOS</strong></li>
</ul>
<p>RADOS (Reliable, Autonomic, Distributed Object Store) 意为可靠、自动、分布式的对象存储，这一层是Ceph的基础，即用来存储用户数据一层。</p><ul>
<li><strong>（2）基础库librados</strong></li>
</ul>
<p>对RADOS抽象、封装，向上层提供API，便于直接基于RADOS进行开发。</p><blockquote>
<p>RADOS采用C++开发，原生librados API包括C和C++两类。librados和基于其开发的应用在物理上位于同一台机器，也因此被称为本地API。应用通过本地调用librados API，再由librados API通过socket与RADOS集群中的节点通信完成操作。</p></blockquote>
<ul>
<li><p><strong>（3）高层应用接口</strong></p><ul>
<li><p><strong>RADOS GW (RADOS Gateway)</strong> ：是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RADOS GW提供的API抽象层次更高，但功能则不如librados强大。因此，开发者应针对自己的需求选择使用。</p></li>
<li><p><strong>RBD (Reliable Block Device)</strong>：提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。</p></li>
</ul>
</li>
<li><p>**（4）Ceph FS **</p></li>
</ul>
<p>Ceph FS(Ceph File System) ，即Ceph集群的文件管理系统。通过Linux内核客户端和FUSE来提供一个兼容POSIX的文件系统。</p><p><strong>RADOS的存储逻辑架构</strong></p><figure style="flex: 73.32848837209302" ><img loading="lazy" width="1009" height="688" src="https://s2.ax1x.com/2020/03/05/372cKs.png" /><figcaption>RADOS系统结构图</figcaption></figure><p>RADOS集群主要有两种节点：</p><ol>
<li>OSD（负责数据存储和维护）</li>
<li>Monitor（系统状态检测和维护）</li>
</ol>
<p>OSD和Monitor之间相互传输节点状态信息，形成Cluster Map。Cluster Map这个数据结构和RADOS提供的特定算法相结合，实现了Ceph“无需查表，算算就好”的核心机制和若干优秀特性。</p><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/63725901">如何“算”出存储位置：CRUSH</a></p></blockquote>
<p><strong>客户端程序通过Cluster Map获取节点状态信息，然后在本地计算出OSD位置，之后直接与OSD通信，完成操作。</strong></p><p>由此可见，只要保证Cluster Map不频繁更新，则客户端完全不需要查表，也不依赖于任何元数据服务器，就可以访问OSD。RADOS运行过程中，Cluster Map的更新取决于系统状态的变化，只有两种：</p><ol>
<li>OSD出现故障。</li>
<li>RADOS规模扩大。正常情况下，这两种事件发生的频率显然远远低于客户端对数据进行访问的频率。</li>
</ol>
<hr />
<h2>2. Ceph工作原理及流程</h2>
<h3>2.1 RADOS对象寻址</h3>
<p>Ceph存储集群，从Ceph客户端接收数据并存储为对象。对象是文件系统中的一个文件，存储在OSD上，由Ceph OSD守护进程处理OSD上的读/写操作。通常来说，一块磁盘和该磁盘对应的守护进程称为一个OSD。</p><p>传统架构中，客户端与一个中心化组件通信（网关、API、中间件等），可能导致单点故障并限制了性能与伸缩性。</p><p>Ceph消除了集中网关，允许客户端直接和Ceph OSD守护进程通讯。同时Ceph会自动在其他Ceph节点创建对象副本来确保数据安全和高可用性。此外，为了保证高可用性，Ceph Monitor也实现了集群化。该功能基于CRUSH算法。</p><figure style="flex: 71.64502164502164" ><img loading="lazy" width="993" height="693" src="https://s2.ax1x.com/2020/03/08/3vIpTK.png" /><figcaption>Ceph寻址流程图</figcaption></figure><ul>
<li>File</li>
</ul>
<p>用户存储/访问的文件。用户存储数据到Ceph集群时，存储数据会被分割成多个object。</p><ul>
<li>object</li>
</ul>
<p>Ceph存储的最小存储单元。每个object都有一个object id，每个object的大小是可以设置的，默认是4MB。</p><ul>
<li>PG</li>
</ul>
<p>Placement Group，对object的存储进行组织和位置映射。PG是一个逻辑概念，类似于数据库中的索引表，它的数量是固定的，不会随着OSD增加或减少而改变。每个object最后都会通过CRUSH计算映射到某个PG中，一个PG可以包含多个object。而PG和OSD之间则是“多对多”的映射关系。进行数据迁移时，Ceph会以PG为单位进行迁移，不会直接操作对象。PG存在的意义是提高ceph存储系统的性能和扩展性。</p><blockquote>
<p>PG是一种间址，PG的数量有限，记录PG跟OSD间的映射关系可行，而记录object到OSD之间的映射因为数量巨大而实际不可行或效率太低。从用途来说，搞个映射本身不是目的，<strong>让故障或者负载均衡变得可操作是目的。</strong></p></blockquote>
<ul>
<li>OSD</li>
</ul>
<p>Object Storage Device，PG通过CRUSH算法映射到OSD中存储。如果是二副本的，则每个PG都会映射到二个osd，比如[osd.1,osd.2]，那么osd.1是存放该PG的主副本，osd.2是存放该PG的从副本，保证了数据的冗余。</p><p><strong>流程</strong></p><figure style="flex: 43.687943262411345" ><img loading="lazy" width="616" height="705" src="https://s2.ax1x.com/2020/03/10/8FDSJg.jpg" /><figcaption>结构</figcaption></figure><ul>
<li>将file切片为多个object，每个object有一个oid。</li>
</ul>
<blockquote>
<p>切割规则本质上就是按照object的大小来切file，优点：</p><ol>
<li>object大小一致便于RADOS高效管理</li>
<li>file的串行处理变成object的并行处理</li>
</ol>
</blockquote>
<blockquote>
<p>oid = ino (identity number) + ono (object number)，ino是file的id，ono是object的id，前者用于文件定位，后者用于文件切片位置的定位。</p></blockquote>
<ul>
<li>一个或多个object映射到PG中，映射规则：<code>oid = hash(oid) % m</code></li>
</ul>
<blockquote>
<p>ceph利用静态hash函数将oid集合映射为近似均匀分布的伪随机值集合，并且集合中每个oid都与PG的数量去模得到PG序号 (PGid)。PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好的提升CEPH集群的性能并使数据均匀分布。</p><ol>
<li>hash完成均匀分布</li>
<li>取模随机选取</li>
</ol>
<p>因为只有大量object和PG，这种伪随机关系的近似均匀才能成立，故要注意两点：</p><ol>
<li>object的size要合理配置，使得file能被切分成足够多的object</li>
<li>PG数量应该为OSD总数的数百倍，才能保证有足够数量的PG提供映射。</li>
</ol>
</blockquote>
<ul>
<li>一个PG映射到n个OSD中（称为n副本，常用3副本），PGid通过CRUSH计算，得到n个OSD，其上运行的OSD deamon负责执行映射到本地的object在本地文件系统中的存储、访问、元数据维护等操作。</li>
</ul>
<h3>2.2 数据的操作流程</h3>
<p>模拟file的写入流程，为便于理解先假设如下前提：</p><ol>
<li>file较小，无需切分，故被映射到一个object上</li>
<li>三副本情况</li>
</ol>
<figure style="flex: 56.31868131868132" ><img loading="lazy" width="410" height="364" src="https://s2.ax1x.com/2020/03/09/8pnLqS.png" /><figcaption>file写入流程</figcaption></figure><p><strong>流程描述</strong></p><ul>
<li>本地RADOS对象寻址，得到OSDs的id</li>
</ul>
<p>client向ceph集群写入file时，先在本地进行RADOS对象寻址，将file转为object，并得到一组的3个OSDid，按照序号大小，分别为Primary OSD, Secondary OSD, Tertiary OSD。</p><ul>
<li>发起写入操作</li>
</ul>
<p>client直接与Primary OSD通信，发起写入操作，Primary OSD接收到之后，并不立即执行写入操作，而先向Secondary OSD, Tertiary OSD发起写入操作，等待确认信息。</p><ul>
<li>回复确认信息</li>
</ul>
<p>Secondary OSD和Tertiary OSD完成写入操作后，回复Primary OSD确认信息，Primary OSD才执行写入操作，并回复client确认信息，至此完成写入操作。</p><blockquote>
<p>等待确认信息的过程会有较大的延时，因此，ceph可以分两次向client回复确认信息。</p><ol>
<li>第一次，数据写入OSD内存缓冲区时，回复client。此时client可继续执行操作。</li>
<li>第二次，数据从缓存写入硬盘后，再回复client。此时client可根据需要删除本地数据，从而完成写入操作。</li>
</ol>
</blockquote>
<p>从上述流程可以看到，client寻址直接在本地完成，无需中心化组件。故client与OSD之间可以多对多并行通信，满足了分布式的要求。并且，OSD在各个PG中可能担任的角色不同，从而尽最大可能均分了工作负担，避免单个OSD的性能问题造成系统的性能瓶颈。</p><p>读取数据同理，client直接和Primary OSD通信，最终由Primary OSD返回读数据。</p><blockquote>
<p>Q&amp;A：</p><ol>
<li>读数据是否可以从任意从OSD读取而非仅仅从主OSD读取？</li>
<li>或者是否可以从n个OSD并行读取呢？从而提高带宽利用率。</li>
</ol>
</blockquote>
<h3>2.3 集群维护</h3>
<p>若干Monitor共同负责了Ceph集群所有OSD状态的发现与记录，并形成Cluster Map，并分发到各个OSD和client中。OSD利用Cluster Map进行数据的维护，client利用Cluster Map进行寻址操作。</p><p>Monitor之间为主从备份关系，Monitor并不主动轮询OSD（这样做时间开销太大），而是OSD主动上报状态信息。Monitor收到信息后更新Cluster Map。触发条件有：</p><ol>
<li>新OSD加入集群</li>
<li>某个OSD发现自身</li>
<li>其他OSD出现异常</li>
</ol>
<p><strong>Cluster Map的组成</strong></p><ul>
<li>Monitor Map</li>
</ul>
<p>包含集群的 <code>fsid</code> 、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监视器图，用 <code>ceph mon dump</code> 命令。</p><ul>
<li>OSD Map</li>
</ul>
<p>包含集群 <code>fsid</code> 、创建时间、最近修改时间、存储池列表、副本数量、归置组数量、 OSD 列表及其状态（如 <code>up</code> 、 <code>in</code> ）。要查看OSD运行图，用 <code>ceph osd dump</code> 命令。</p><p>OSD状态的描述分为两个维度：up或者down（表明OSD是否正常工作），in或者out（表明OSD是否承载PG）。因此，对于任意一个OSD，共有四种可能的状态：</p><p>—— Up且in：说明该OSD正常运行，且已经承载至少一个PG的数据。这是一个OSD的标准工作状态；</p><p>—— Up且out：说明该OSD正常运行，但并未承载任何PG，其中也没有数据。一个新的OSD刚刚被加入Ceph集群后，便会处于这一状态。而一个出现故障的OSD被修复后，重新加入Ceph集群时，也是处于这一状态；</p><p>—— Down且in：说明该OSD发生异常，但仍然承载着至少一个PG，其中仍然存储着数据。这种状态下的OSD刚刚被发现存在异常，可能仍能恢复正常，也可能会彻底无法工作；</p><p>—— Down且out：说明该OSD已经彻底发生故障，且已经不再承载任何PG。</p><ul>
<li>PG Map</li>
</ul>
<p>包含归置组版本、其时间戳、最新的 OSD 运行图版本、占满率、以及各归置组详情，像归置组 ID 、 up set 、 acting set 、 PG 状态（如 <code>active+clean</code> ），和各存储池的数据使用情况统计。</p><ul>
<li>CRUSH Map</li>
</ul>
<p>包含存储设备列表、故障域树状结构（如设备、主机、机架、行、房间、等等）、和存储数据时如何利用此树状结构的规则。要查看 CRUSH 规则，执行 <code>ceph osd getcrushmap -o {filename}</code> 命令；然后用 <code>crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}</code> 反编译；然后就可以用 <code>cat</code> 或编辑器查看了。</p><ul>
<li>MDS Map</li>
</ul>
<p>包含当前 MDS 图的版本、创建时间、最近修改时间，还包含了存储元数据的存储池、元数据服务器列表、还有哪些元数据服务器是 <code>up</code> 且 <code>in</code> 的。要查看 MDS 图，执行 <code>ceph mds dump</code> 。</p><h3>2.4 增加OSD流程</h3>
<ul>
<li>新OSD上线后，如果PG处于正常状态，则新OSD首先根据配置信息与monitor通信。Monitor将其加入cluster map，并设置为up且out状态，再将最新版本的cluster map发给这个新OSD。收到monitor发来的cluster map之后，这个新OSD计算出自己所承载的PG（为简化讨论，此处我们假定这个新的OSD开始只承载一个PG），以及和自己承载同一个PG的其他OSD。然后，新OSD将与这些OSD取得联系。如图：</li>
</ul>
<figure style="flex: 74.86535008976661" ><img loading="lazy" width="834" height="557" src="https://s2.ax1x.com/2020/03/09/8pYyu9.png" /><figcaption>PG处于正常状态，OSD未启用</figcaption></figure><ul>
<li>新OSD上线后，如果PG处于降级状态（即承载该PG的OSD个数少于正常值，如正常应该是3个，此时只有2个或1个。这种情况通常是OSD故障所致），则其他OSD将把这个PG内的所有对象和元数据复制给新OSD。数据复制完成后，新OSD被置为up且in状态。而cluster map内容也将据此更新。这事实上是一个自动化的failure recovery过程。当然，即便没有新的OSD加入，降级的PG也将计算出其他OSD实现failure recovery，如图：</li>
</ul>
<figure style="flex: 73.29093799682035" ><img loading="lazy" width="922" height="629" src="https://s2.ax1x.com/2020/03/09/8ptQV1.png" /><figcaption>PG处于降级状态时</figcaption></figure><ul>
<li>如果该PG目前一切正常，则这个新OSD将替换掉现有OSD中的一个（PG内将重新选出Primary OSD），并承担其数据。在数据复制完成后，新OSD被置为up且in状态，而被替换的OSD将退出该PG（但状态通常仍然为up且in，因为还要承载其他PG）。而cluster map内容也将据此更新。这事实上是一个自动化的数据re-balancing过程，如图：</li>
</ul>
<figure style="flex: 89.0909090909091" ><img loading="lazy" width="1078" height="605" src="https://s2.ax1x.com/2020/03/09/8ptaqA.png" /><figcaption>PG处于正常状态，新旧OSD更换</figcaption></figure><hr />
<h2>3. Ceph (Luminous)集群创建</h2>
<p>一个Ceph集群至少需要3个OSD才能实现冗余和高可用性，并且至少需要一个mon和一个mds。其中mon和mds可以部署在其中一个osd节点上，详细可参考<a href="http://docs.ceph.org.cn/start/">官方安装文档</a>。</p><h3>3.1 服务器初始化</h3>
<p><strong>不要用CentOS 8！！！</strong></p><p><strong>不要用CentOS 8！！！</strong></p><p><strong>不要用CentOS 8！！！</strong></p><p><strong>不要问我怎么知道的，全是泪……</strong></p><p>官方文档给出了建议的<a href="http://docs.ceph.org.cn/start/os-recommendations/#id2">操作系统</a>，不看官方文档的后果就是…吐血</p><p>本来想用虚拟机的，后来想到我在Vultr上还有100$赠送券没用，索性直接用VPS来实现了。</p><p>结果Vultr给我埋了个坑！厂商初始化的系统，只有一个vda磁盘，且所有的容量都挂载根分区了。而Ceph需要一个格式为xfs的单独分区，搜了下linux缩小根分区要进rescue模式，VPS又不好弄…</p><p>解铃还须系铃人，搜了半天，最终找到解决办法了。就是使用VPS厂商提供的<strong>Block Storage</strong>功能来扩展出一个分区。不过最蛋疼的是这个功能支持<strong>纽约</strong>机房的VPS，之前搞了半天西雅图的机房，又得重来...裂开了</p><p>扩展好分区之后，硬盘情况如下：</p><figure style="flex: 79.37158469945355" ><img loading="lazy" width="581" height="366" src="https://s2.ax1x.com/2020/03/10/8i3wwR.png" /><figcaption>硬盘分区情况</figcaption></figure><p>各个节点配置如下：</p><table>
<thead>
<tr>
  <th>主机名</th>
  <th>系统</th>
  <th>IP</th>
  <th>数据磁盘</th>
  <th>备注</th>
</tr>
</thead>
<tbody>
<tr>
  <td>ceph-client</td>
  <td>CentOS 7 X64</td>
  <td>140.82.15.88</td>
  <td>25G</td>
  <td>client</td>
</tr>
<tr>
  <td>ceph-node1</td>
  <td>CentOS 7 X64</td>
  <td>104.207.133.105</td>
  <td>25G</td>
  <td>mgr.mon.osd.0</td>
</tr>
<tr>
  <td>ceph-node2</td>
  <td>CentOS 7 X64</td>
  <td>45.63.5.140</td>
  <td>25G</td>
  <td>osd.1</td>
</tr>
<tr>
  <td>ceph-node3</td>
  <td>CentOS 7 X64</td>
  <td>149.28.40.16</td>
  <td>25G</td>
  <td>osd.2</td>
</tr>
</tbody>
</table>
<blockquote>
<p>mgr：管理节点</p><p>mon：监控节点</p><p>osd：存储节点</p><p>Ceph集群要求必须是奇数个mon监控节点，一般建议至少是3个</p><p>Ceph的文件系统作为一个目录挂载到客户端cephclient的<code>/cephfs</code>目录下，可以像操作普通目录一样对此目录进行操作。</p></blockquote>
<h3>3.2 环境准备</h3>
<p>ok，话不多说，先把环境配好。</p><h4>(1) 修改hostname并绑定主机名映射</h4>
<p>这一步是为了统一名称，方便后面命令操作使用的。</p><p><strong>在4个主机上修改hostname</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph1 ~<span class="o">]</span><span class="c1"># hostnamectl set-hostname ceph-client</span>
<span class="o">[</span>root@ceph2 ~<span class="o">]</span><span class="c1"># hostnamectl set-hostname ceph-node1</span>
<span class="o">[</span>root@ceph3 ~<span class="o">]</span><span class="c1"># hostnamectl set-hostname ceph-node2</span>
<span class="o">[</span>root@ceph4 ~<span class="o">]</span><span class="c1"># hostnamectl set-hostname ceph-node3</span>
</pre></div>
<blockquote>
<p>如果使用ssh登陆的远程VPS，设置之后可能没有刷新hostname，exit退出重新登陆即可</p></blockquote>
<p><strong>在3个节点机上添加hosts</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># vi /etc/hosts</span>

<span class="m">104</span>.207.133.105 ceph-node1
<span class="m">45</span>.63.5.140 ceph-node2
<span class="m">149</span>.28.40.16 ceph-node3
</pre></div>
<p>连通性测试</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-client ~<span class="o">]</span><span class="c1"># ping -c 2 ceph-node1</span>
<span class="o">[</span>root@ceph-client ~<span class="o">]</span><span class="c1"># ping -c 2 ceph-node2</span>
<span class="o">[</span>root@ceph-client ~<span class="o">]</span><span class="c1"># ping -c 2 ceph-node3</span>
</pre></div>
<figure style="flex: 150.87719298245614" ><img loading="lazy" width="688" height="228" src="https://s2.ax1x.com/2020/03/10/8CgorR.png" /><figcaption>连通性测试成功</figcaption></figure><p>另外2个节点机同理执行如上操作</p><h4>(2) 创建ceph用户</h4>
<p>我们一般不会再root下工作，所以创建一个<code>ceph</code>用户来使用。</p><p><strong>在3个节点机上创建用户ceph，密码统一为ceph</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># adduser ceph &amp;&amp; echo &quot;ceph:ceph&quot;|chpasswd</span>
</pre></div>
<p>为每个ceph节点用户增加root权限</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># echo &quot;ceph ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph &amp;&amp; chmod 0440 /etc/sudoers.d/ceph</span>
</pre></div>
<p>测试root权限，<code>exit</code>退回切换前用户</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># su - ceph </span>
<span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span><span class="c1"># sudo su -</span>
</pre></div>
<h4>(3) 安装NTP</h4>
<p>建议3个节点机安装NTP，以免因时钟漂移导致故障（mon节点必须）</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># yum install ntp ntpdate ntp-doc -y </span>
<span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># systemctl restart ntpd</span>
<span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># systemctl status ntpd</span>
</pre></div>
<h4>(4) 关闭防火墙和Selinux</h4>
<p>保证各个节点之间的联通，在开发阶段完全可以关闭防火墙，之后再重新打开配置。</p><p>SELinux是一种基于 域-类型 模型（domain-type）的强制访问控制（MAC）安全系统，说人话就是Linux的一个安全子系统，它是默认开启的 ，会对我们Ceph安装过程造成一定的影响，所以先关掉它。</p><p><strong>关闭防火墙</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># systemctl stop firewalld</span>
<span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># systemctl disable firewalld</span>
</pre></div>
<p><strong>关闭selinux</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config &amp;&amp; setenforce 0</span>
</pre></div>
<h4>(5) epel源配置</h4>
<p>yum默认的源是国外的，因为众所周知的原因，它的速度可能会比较慢。所以我们换成国内的源。</p><p><strong>把软件包源加入软件仓库</strong></p><p>用文本编辑器创建一个 YUM 库文件：</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># vi /etc/yum.repos.d/ceph.repo</span>

<span class="o">[</span>ceph<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>Ceph packages <span class="k">for</span> <span class="nv">$basearch</span>
<span class="nv">baseurl</span><span class="o">=</span>http://mirrors.aliyun.com/ceph/rpm-luminous/el7/<span class="nv">$basearch</span>
<span class="nv">enabled</span><span class="o">=</span><span class="m">1</span>
<span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
<span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
<span class="nv">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://mirrors.aliyun.com/ceph/keys/release.asc

<span class="o">[</span>ceph-noarch<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>Ceph noarch packages
<span class="nv">baseurl</span><span class="o">=</span>http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch
<span class="nv">enabled</span><span class="o">=</span><span class="m">1</span>
<span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
<span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
<span class="nv">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://mirrors.aliyun.com/ceph/keys/release.asc

<span class="o">[</span>ceph-source<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>Ceph <span class="nb">source</span> packages
<span class="nv">baseurl</span><span class="o">=</span>http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS
<span class="nv">enabled</span><span class="o">=</span><span class="m">0</span>
<span class="nv">gpgcheck</span><span class="o">=</span><span class="m">1</span>
<span class="nv">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://mirrors.aliyun.com/ceph/keys/release.asc
<span class="nv">priority</span><span class="o">=</span><span class="m">1</span>
</pre></div>
<p>缓存服务器的包信息</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># yum makecache</span>
</pre></div>
<p>更新yum</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># yum update -y</span>
</pre></div>
<h4>(5) mgr节点安装ceph-deploy</h4>
<p>之前的操作，都需要在所有非客户端节点进行，这个只需要在作为mgr节点的<code>ceph-node1</code>上操作即可。</p><p><strong>安装ceph-deploy管理工具</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span><span class="c1"># yum install -y ceph-deploy</span>
</pre></div>
<figure style="flex: 107.83582089552239" ><img loading="lazy" width="578" height="268" src="https://s2.ax1x.com/2020/03/10/8PmlKx.png" /><figcaption>ceph-deploy部署工具安装成功</figcaption></figure><h4>(6) 配置SSH服务器</h4>
<p>正因为 <code>ceph-deploy</code> 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点，完成节点间的免密码登陆。</p><p><strong>管理节点添加到各个节点机之间配置ssh服务器</strong></p><p>根据官方文档建议，不要使用<code>root</code>用户或<code>sudo</code>，切换回<code>ceph</code>用户。</p><div class="highlight"><pre><span></span><span class="o">[</span>root@ceph-node1 ~<span class="o">]</span>$ su - ceph
</pre></div>
<p>新建密钥（一路回车即可）</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ ssh-keygen
</pre></div>
<p>公钥分发，期间提示输入其他节点<code>ceph</code>用户的密码，我们同样也是配置的<code>ceph</code></p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ ssh-copy-id  ceph@ceph-node2
<span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ ssh-copy-id  ceph@ceph-node3
</pre></div>
<p><strong>配置.ssh/config文件</strong></p><p>修改 ceph-deploy 管理节点<code>ceph-node1</code>上的 <code>~/.ssh/config</code>文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 –username {username} 。这样做同时也简化了 ssh 的用法。</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ vi ~/.ssh/config

Host node1
   Hostname ceph-node1
   User ceph
Host node2
   Hostname ceph-node2
   User ceph
Host node3
   Hostname ceph-node3
   User ceph
</pre></div>
<blockquote>
<p>Host后为ssh连接的名称</p><p>Hostname接host文件中的名字，也是ceph-deploy所用的名字</p></blockquote>
<p>重启ssh服务（需要输入root权限的密码）</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ sudo chmod <span class="m">600</span> .ssh/config <span class="o">&amp;&amp;</span> systemctl restart sshd
</pre></div>
<p>测试ssh连接</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ ssh node2
<span class="o">[</span>ceph@ceph-node2 ~<span class="o">]</span>$ <span class="nb">exit</span>
</pre></div>
<h3>3.3 创建集群</h3>
<h4>(1) 创建ceph工作目录并配置ceph.conf</h4>
<p>由于<code>ceph-deploy</code>会在当前目录产生文件，故新建一个目录。</p><p><strong>在mgr节点上创建ceph工作目录</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ mkdir my-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd</span> my-cluster
</pre></div>
<p>创建ceph集群，<code>new</code>后接mon节点的主机名。</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy new ceph-node1
</pre></div>
<figure style="flex: 201.11111111111111" ><img loading="lazy" width="724" height="180" src="https://s2.ax1x.com/2020/03/10/8iyORe.png" /><figcaption>安装成功</figcaption></figure><blockquote>
<p>如果遇到python的import error：</p><p>这个问题通常是由于升级到python2.7后执行pip产生的，解决方案是重新在python2.7环境中安装pip，或者安装setuptools即可：</p></blockquote>
<div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ sudo yum install -y python-setuptools
</pre></div>
<p><code>ceph-deploy</code>的<code>new</code>子命令能够部署一个默认名称为<code>ceph</code>的新集群，并且它能生成集群配置文件和密钥文件。列出当前工作目录，你会看到<code>ceph.conf</code>和<code>ceph.mon.keyring</code>文件。</p><figure style="flex: 288.8" ><img loading="lazy" width="722" height="125" src="https://s2.ax1x.com/2020/03/10/8icPmR.png" /><figcaption>产生的文件</figcaption></figure><blockquote>
<p>如果需要(新安装的系统通常不需要)，部署之前确保ceph每个节点没有ceph数据包（先清空之前所有的ceph数据，如果是新装不用执行此步骤，如果是重新部署的话也执行下面的命令）</p></blockquote>
<div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1<span class="o">]</span>$ ceph-deploy purge ceph-deploy ceph-u0-m0 ceph-u0-l0 ceph-u0-r0
<span class="o">[</span>ceph@ceph-node1<span class="o">]</span>$ ceph-deploy purgedata ceph-deploy ceph-u0-m0 ceph-u0-l0 ceph-u0-r0
<span class="o">[</span>ceph@ceph-node1<span class="o">]</span>$ ceph-deploy forgetkeys
</pre></div>
<p><strong>配置监控节点ceph.conf</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ vi ceph.conf
<span class="c1"># 修改mon_host</span>
<span class="nv">mon_host</span> <span class="o">=</span> <span class="m">104</span>.207.133.105		//监控节点ip
</pre></div>
<p><a href="http://docs.ceph.org.cn/rados/configuration/ceph-conf/">ceph.conf的配置文档</a></p><h4>(2) 节点安装ceph</h4>
<p>在管理节点，利用ceph-deploy工具为各个节点安装ceph</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy install ceph-node1 ceph-node2 ceph-node3
</pre></div>
<p>在各个节点检查安装情况</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 ~<span class="o">]</span>$ ceph --version
<span class="o">[</span>ceph@ceph-node2 ~<span class="o">]</span>$ ceph --version
<span class="o">[</span>ceph@ceph-node3 ~<span class="o">]</span>$ ceph --version
</pre></div>
<figure style="flex: 218.9655172413793" ><img loading="lazy" width="889" height="203" src="https://s2.ax1x.com/2020/03/10/8iWt5q.png" /><figcaption>安装情况检查</figcaption></figure><h4>(3) mon节点初始化</h4>
<p>初始化的时候，会生成mon节点监测集群所使用的秘钥</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy mon create-initial
</pre></div>
<blockquote>
<p>本文在管理节点安装的mon，如果需要在其他节点安装mon，可使用如下命令：</p></blockquote>
<div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy mon node1 node2 node3 node4
</pre></div>
<p><strong>将管理节点的配置分发到其它节点</strong></p><p>收集所有密钥，OSD添加好后，用 <code>ceph-deploy</code> 把配置文件和 管理节点密钥拷贝到管理节点和 Ceph 节点，这样每次执行 Ceph 命令行时就无需指定 mon节点 地址和 <code>ceph.client.admin.keyring</code> 了</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3
</pre></div>
<p>将mon节点生成的密钥环分发到各个节点</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ sudo scp *.keyring ceph-node1:/etc/ceph/
<span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ sudo scp *.keyring ceph-node2:/etc/ceph/
<span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ sudo scp *.keyring ceph-node3:/etc/ceph/
</pre></div>
<p>允许<code>ceph</code>用户访问<code>etc/ceph</code>目录下的文件</p><div class="highlight"><pre><span></span>sudo chown -R ceph:ceph /etc/ceph
</pre></div>
<p>输入<code>ceph -s</code>查看当前集群的状态信息</p><figure style="flex: 64.59537572254335" ><img loading="lazy" width="447" height="346" src="https://s2.ax1x.com/2020/03/11/8AFMNQ.png" /><figcaption>ceph集群状态</figcaption></figure><h4>(4) 配置mgr</h4>
<p>mgr (manager)，用于管理集群</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy mgr create ceph-node1
</pre></div>
<p>开启仪表盘</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph mgr module <span class="nb">enable</span> dashboard
</pre></div>
<p>通过<code>http://104.207.133.105:7000/</code>查看集群信息</p><figure style="flex: 146.7075038284839" ><img loading="lazy" width="1916" height="653" src="https://s2.ax1x.com/2020/03/11/8AVFVf.png" /><figcaption>管理界面</figcaption></figure><h4>(5) 添加OSD到集群</h4>
<p><strong>检查OSD节点上所有可用的磁盘</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy disk list ceph-node1 ceph-node2 ceph-node3
</pre></div>
<p><strong>使用zap选项删除所有osd节点上的分区</strong></p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy disk zap ceph-node1 /dev/vdb
<span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy disk zap ceph-node2 /dev/vdb
<span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy disk zap ceph-node3 /dev/vdb
</pre></div>
<blockquote>
<p>Ceph12版本以后prepare和activate命令已经没有了，用create替代</p></blockquote>
<p>准备OSD（使用prepare命令）</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy osd prepare ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2
</pre></div>
<p>激活OSD（使用activate命令）</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy osd activate ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2
</pre></div>
<p><strong>创建OSD</strong></p><p>使用<code>ceph-deploy osd create --data {device} {ceph-node}</code>创建OSD并添加到集群中</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/vdb ceph-node1
<span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/vdb ceph-node2
<span class="o">[</span>ceph@ceph-1 cluster<span class="o">]</span>$ ceph-deploy osd create --data /dev/vdb ceph-node3
</pre></div>
<figure style="flex: 68.31395348837209" ><img loading="lazy" width="470" height="344" src="https://s2.ax1x.com/2020/03/11/8An9fS.png" /><figcaption>添加OSD成功</figcaption></figure><p>ok，至此完工！可以去浏览器仪表盘直观地查看部署后的情况。</p><h3>3.4 集群管理</h3>
<p>如果之前部署出了问题，可以用下面的命令予以修正。</p><p>检查集群状态</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ sudo ceph health
</pre></div>
<p>清除配置</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy purgedata ceph-node1 ceph-node2 ceph-node3
<span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy forgetkeys
</pre></div>
<p>清除ceph包</p><div class="highlight"><pre><span></span><span class="o">[</span>ceph@ceph-node1 cluster ~<span class="o">]</span>$ ceph-deploy purge ceph-node1 ceph-node2 ceph-node3
</pre></div>
<hr />
<h2>引用</h2>
<p><a href="https://www.cnblogs.com/kevingrace/p/8387999.html">Ceph分布式存储工作原理 及 部署介绍-散尽浮华</a></p><p><a href="https://www.cnblogs.com/kevingrace/p/9141432.html">Ceph-deploy快速部署Ceph分布式存储</a></p><p><a href="https://www.cnblogs.com/linuxk/category/1267134.html">Ceph学习之路-烟雨浮华</a></p><p><a href="https://www.cnblogs.com/passzhang/p/12151835.html">全网最详细的Ceph14.2.5集群部署及配置文件详解-PassZhang</a></p><p><a href="https://blog.51cto.com/6854290/2361910">Centos7.6安装Ceph（luminous）-会飞的冬瓜</a></p>
            </div>
        </article>
        <div id="ga-tags">
    
</div>
    </section>

    
<section id="ga-content_pager">

    <div class="next">
        <a class="ga-highlight" href="https://blog.imadamtang.cn/archives/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">吴恩达机器学习笔记</a>
        <p class="yue">1 概述</p>
    </div>


    <div class="prev">
        <h3>没有了</h3>
        <p class="yue">这是最旧的文章</p>
    </div>

</section>


    
        <script>
            var initValine = function () {
                new Valine({"enable": true, "el": "#vcomments", "appId": "whpJbIrUvboS1qOUGbAQLYMO-gzGzoHsz", "appKey": "XJmv8dJpt1RJltuKYpw5Io9P", "visitor": true, "recordIP": true});
            }
        </script>
        <script defer src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js' onload="initValine()"></script>
        <div id="vcomments"></div>
    

</main>

                <footer class="ga-mono" id="ga-footer">
                    <section>
                        <span id="ga-uptime"></span>
                        <span class="brand">一博客</span>
                    </section>
                    <section>
<!--                        <p class="copyright">-->
<!--                            <span>Copyright © 2023 Adam</span>-->
<!--&lt;!&ndash;                            <span>Powered by <a no-style href="https://github.com/AlanDecode/Maverick" target="_blank">Maverick & Galileo</a></span>&ndash;&gt;-->
<!--                        </p>-->
                        <div class="copyright">
                            <span class="footer-addon">
                                
                            </span>
                            <nav class="social-links">
                                <ul><li><a class="no-style" title="GitHub" href="https://github.com/imadamtang" target="_blank"><i class="gi gi-github"></i>GitHub</a></li></ul>
                            </nav>
                        </div>
                    </section>
                    <script>
                        var site_build_date = "2022-02-22T22:22+08:00"
                    </script>
                    <script src="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/galileo-7c8cea54ab.js"></script>
                </footer>
            </div>
        </div>
    </div>

    <!--katex-->
    <script defer src="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/katex.min.js"></script>
    <script>
    mathOpts = {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "\\[", right: "\\]", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false}
        ]
    };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/auto-render.min.js" onload="renderMathInElement(document.body, mathOpts);"></script>

    <script src="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/ExSearch/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/imadamtang/AdamBlog@main/assets/ExSearch/ExSearch-493cb9cd89.js"></script>

    
    </body>
</html>